{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameters Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    # courtsey http://stackoverflow.com/users/190280/josh-bleecher-snyder\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_Xt_Yt(X, y, percentage=0.95):\n",
    "    p = int(len(X) * percentage)\n",
    "    X_train = X[0:p]\n",
    "    Y_train = y[0:p]\n",
    "     \n",
    "    X_train, Y_train = shuffle_in_unison(X_train, Y_train)\n",
    " \n",
    "    X_test = X[p:]\n",
    "    Y_test = y[p:]\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_nan_examples(data):\n",
    "    newX = []\n",
    "    for i in range(len(data)):\n",
    "        if np.isnan(data[i]).any() == False:\n",
    "            newX.append(data[i])\n",
    "    return newX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Permute, Reshape\n",
    "from keras.layers import Merge, Input, concatenate\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, RepeatVector, AveragePooling1D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.optimizers import RMSprop, Adam, SGD, Nadam\n",
    "from keras.initializers import *\n",
    "from keras.constraints import *\n",
    "from keras import regularizers\n",
    "from keras import losses\n",
    "\n",
    "from keras import backend as K\n",
    "import seaborn as sns\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STEP = 1\n",
    "FORECAST = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_original = pd.read_csv('dataset/700_data_from_IB_1day.csv')\n",
    "data_original = data_original.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAewAAAFVCAYAAAAt79zdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c7b64a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data_original['close'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(window):\n",
    "\n",
    "    openp = data_original.loc[:, 'open'].tolist()[1:]\n",
    "    highp = data_original.loc[:, 'high'].tolist()[1:]\n",
    "    lowp = data_original.loc[:, 'low'].tolist()[1:]\n",
    "    closep = data_original.loc[:, 'close'].tolist()[1:]\n",
    "    volumep = data_original.loc[:, 'volume'].tolist()[1:]\n",
    "    volumecp = data_original.loc[:, 'volume'].tolist()[1:]\n",
    "\n",
    "    volatility = pd.DataFrame(closep).rolling(window).std().values.tolist()\n",
    "    volatility = [v[0] for v in volatility]\n",
    "\n",
    "    X, Y = [], []\n",
    "    for i in range(0, len(data_original), STEP): \n",
    "        try:\n",
    "            o = openp[i:i+window]\n",
    "            h = highp[i:i+window]\n",
    "            l = lowp[i:i+window]\n",
    "            c = closep[i:i+window]\n",
    "            v = volumep[i:i+window]\n",
    "            vc = volumecp[i:i+window]\n",
    "            volat = volatility[i:i+window]\n",
    "\n",
    "            o = (np.array(o) - np.mean(o)) / np.std(o)\n",
    "            h = (np.array(h) - np.mean(h)) / np.std(h)\n",
    "            l = (np.array(l) - np.mean(l)) / np.std(l)\n",
    "            c = (np.array(c) - np.mean(c)) / np.std(c)\n",
    "            v = (np.array(v) - np.mean(v)) / np.std(v)\n",
    "            vc = (np.array(vc) - np.mean(vc)) / np.std(vc)\n",
    "            volat = (np.array(volat) - np.mean(volat)) / np.std(volat)\n",
    "\n",
    "            x_i = np.column_stack((o, h, l, c, v, vc, volat))\n",
    "            x_i = x_i.flatten()\n",
    "\n",
    "            y_i = (closep[i+window+FORECAST] - closep[i+window]) / closep[i+window]\n",
    "\n",
    "            if np.isnan(x_i).any():\n",
    "                continue\n",
    "\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "        X.append(x_i)\n",
    "        Y.append(y_i)\n",
    "\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    X_train, X_test, Y_train, Y_test = create_Xt_Yt(X, Y)\n",
    "    return X_train, X_test, Y_train, Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(params):\n",
    "    X_train, X_test, Y_train, Y_test = prepare_data(params['window'])\n",
    "\n",
    "    print ('Trying', params)\n",
    "\n",
    "\n",
    "    try:\n",
    "        main_input = Input(shape=(len(X_train[0]), ), name='main_input')\n",
    "        x = Dense(params['units1'], activation=params['activation'])(main_input)\n",
    "        x = Dense(params['units2'], activation=params['activation'])(x)\n",
    "        x = Dense(params['units3'], activation=params['activation'])(x)\n",
    "\n",
    "        output = Dense(1, activation = \"linear\", name = \"out\")(x)\n",
    "        final_model = Model(inputs=[main_input], outputs=[output])\n",
    "        opt = Adam(lr=params['lr'])\n",
    "\n",
    "        final_model.compile(optimizer=opt,  loss=params['loss'])\n",
    "\n",
    "        history = final_model.fit(X_train, Y_train, \n",
    "                  epochs = 5, \n",
    "                  batch_size = 256, \n",
    "                  verbose=1, \n",
    "                  validation_data=(X_test, Y_test),\n",
    "                  shuffle=True)\n",
    "\n",
    "        pred = final_model.predict(X_test)\n",
    "\n",
    "    except:\n",
    "        print ('Something happened')\n",
    "        print ('-' * 10)\n",
    "\n",
    "        return {'loss': 999999, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "    predicted = pred\n",
    "    original = Y_test\n",
    "\n",
    "    mse = np.mean(np.square(predicted - original))    \n",
    "\n",
    "    if np.isnan(mse):\n",
    "        print ('NaN happened')\n",
    "        print ('-' * 10)\n",
    "        return {'loss': 999999, 'status': STATUS_OK}\n",
    "\n",
    "    print (mse)\n",
    "    print ('-' * 10)\n",
    "\n",
    "    sys.stdout.flush() \n",
    "    return {'loss': mse, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "space = {'window': hp.choice('window',[30, 60, 120, 180]),\n",
    "        'units1': hp.choice('units1', [64, 512]),\n",
    "        'units2': hp.choice('units2', [64, 512]),\n",
    "        'units3': hp.choice('units3', [64, 512]),\n",
    "\n",
    "        'lr': hp.choice('lr',[0.01, 0.001, 0.0001]),\n",
    "        'activation': hp.choice('activation',['relu',\n",
    "                                                'sigmoid',\n",
    "                                                'tanh',\n",
    "                                                'linear']),\n",
    "        'loss': hp.choice('loss', [losses.logcosh,\n",
    "                                    losses.mse,\n",
    "                                    losses.mae,\n",
    "                                    losses.mape])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying {'activation': 'tanh', 'loss': <function mean_absolute_percentage_error at 0x1103bbae8>, 'lr': 0.0001, 'units1': 64, 'units2': 64, 'units3': 512, 'window': 120}\n",
      "Train on 106 samples, validate on 6 samples\n",
      "Epoch 1/5\n",
      "106/106 [==============================] - 0s - loss: 1127.1788 - val_loss: 1839.2350\n",
      "Epoch 2/5\n",
      "106/106 [==============================] - 0s - loss: 880.1234 - val_loss: 1698.6063\n",
      "Epoch 3/5\n",
      "106/106 [==============================] - 0s - loss: 776.6573 - val_loss: 1512.0699\n",
      "Epoch 4/5\n",
      "106/106 [==============================] - 0s - loss: 733.7391 - val_loss: 1322.2382\n",
      "Epoch 5/5\n",
      "106/106 [==============================] - 0s - loss: 674.0878 - val_loss: 1328.8489\n",
      "0.142258175775\n",
      "----------\n",
      "Trying {'activation': 'linear', 'loss': <function mean_squared_error at 0x1103bb9d8>, 'lr': 0.01, 'units1': 64, 'units2': 64, 'units3': 64, 'window': 120}\n",
      "Train on 106 samples, validate on 6 samples\n",
      "Epoch 1/5\n",
      "106/106 [==============================] - 0s - loss: 2.2146 - val_loss: 407.7226\n",
      "Epoch 2/5\n",
      "106/106 [==============================] - 0s - loss: 686.8295 - val_loss: 304.5903\n",
      "Epoch 3/5\n",
      "106/106 [==============================] - 0s - loss: 574.3662 - val_loss: 103.1729\n",
      "Epoch 4/5\n",
      "106/106 [==============================] - 0s - loss: 102.2657 - val_loss: 243.2410\n",
      "Epoch 5/5\n",
      "106/106 [==============================] - 0s - loss: 345.7610 - val_loss: 1.6291\n",
      "1.63343519434\n",
      "----------\n",
      "Trying {'activation': 'tanh', 'loss': <function logcosh at 0x1103bbd90>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 120}\n",
      "Train on 106 samples, validate on 6 samples\n",
      "Epoch 1/5\n",
      "106/106 [==============================] - 0s - loss: 0.0476 - val_loss: 0.0373\n",
      "Epoch 2/5\n",
      "106/106 [==============================] - 0s - loss: 0.0775 - val_loss: 0.0115\n",
      "Epoch 3/5\n",
      "106/106 [==============================] - 0s - loss: 0.0356 - val_loss: 0.0291\n",
      "Epoch 4/5\n",
      "106/106 [==============================] - 0s - loss: 0.0385 - val_loss: 0.0422\n",
      "Epoch 5/5\n",
      "106/106 [==============================] - 0s - loss: 0.0469 - val_loss: 0.0261\n",
      "0.0548035249162\n",
      "----------\n",
      "Trying {'activation': 'linear', 'loss': <function mean_absolute_percentage_error at 0x1103bbae8>, 'lr': 0.001, 'units1': 64, 'units2': 64, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 5766363.0000 - val_loss: 15099.5166\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 26296614.0000 - val_loss: 11589.1357\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 19566056.0000 - val_loss: 4827.9976\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 5985898.0000 - val_loss: 2279.3518\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 4805084.5000 - val_loss: 4776.3442\n",
      "0.508977094087\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_absolute_percentage_error at 0x1103bbae8>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 120}\n",
      "Train on 106 samples, validate on 6 samples\n",
      "Epoch 1/5\n",
      "106/106 [==============================] - 0s - loss: 645.7895 - val_loss: 408.5263\n",
      "Epoch 2/5\n",
      "106/106 [==============================] - 0s - loss: 963.9050 - val_loss: 364.3969\n",
      "Epoch 3/5\n",
      "106/106 [==============================] - 0s - loss: 871.7863 - val_loss: 85.9409\n",
      "Epoch 4/5\n",
      "106/106 [==============================] - 0s - loss: 270.3316 - val_loss: 368.5637\n",
      "Epoch 5/5\n",
      "106/106 [==============================] - 0s - loss: 663.5823 - val_loss: 491.4871\n",
      "0.0207321746707\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_absolute_percentage_error at 0x1103bbae8>, 'lr': 0.01, 'units1': 512, 'units2': 64, 'units3': 64, 'window': 180}\n",
      "Something happened\n",
      "----------\n",
      "Trying {'activation': 'tanh', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.001, 'units1': 512, 'units2': 64, 'units3': 512, 'window': 180}\n",
      "Something happened\n",
      "----------\n",
      "Trying {'activation': 'linear', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.01, 'units1': 64, 'units2': 512, 'units3': 64, 'window': 30}\n",
      "Train on 277 samples, validate on 15 samples\n",
      "Epoch 1/5\n",
      "277/277 [==============================] - 0s - loss: 1.8785 - val_loss: 33.8982\n",
      "Epoch 2/5\n",
      "277/277 [==============================] - 0s - loss: 24.4908 - val_loss: 22.2380\n",
      "Epoch 3/5\n",
      "277/277 [==============================] - 0s - loss: 15.8506 - val_loss: 16.1307\n",
      "Epoch 4/5\n",
      "277/277 [==============================] - 0s - loss: 8.6376 - val_loss: 3.8455\n",
      "Epoch 5/5\n",
      "277/277 [==============================] - 0s - loss: 2.8065 - val_loss: 10.1998\n",
      "114.973705338\n",
      "----------\n",
      "Trying {'activation': 'tanh', 'loss': <function logcosh at 0x1103bbd90>, 'lr': 0.001, 'units1': 512, 'units2': 64, 'units3': 64, 'window': 120}\n",
      "Train on 106 samples, validate on 6 samples\n",
      "Epoch 1/5\n",
      "106/106 [==============================] - 0s - loss: 0.1928 - val_loss: 2.3045\n",
      "Epoch 2/5\n",
      "106/106 [==============================] - 0s - loss: 2.4492 - val_loss: 1.2336\n",
      "Epoch 3/5\n",
      "106/106 [==============================] - 0s - loss: 1.1884 - val_loss: 0.3295\n",
      "Epoch 4/5\n",
      "106/106 [==============================] - 0s - loss: 0.2358 - val_loss: 0.0052\n",
      "Epoch 5/5\n",
      "106/106 [==============================] - 0s - loss: 0.0077 - val_loss: 0.0519\n",
      "0.105596744158\n",
      "----------\n",
      "Trying {'activation': 'linear', 'loss': <function mean_squared_error at 0x1103bb9d8>, 'lr': 0.0001, 'units1': 64, 'units2': 64, 'units3': 512, 'window': 120}\n",
      "Train on 106 samples, validate on 6 samples\n",
      "Epoch 1/5\n",
      "106/106 [==============================] - 0s - loss: 0.6545 - val_loss: 0.0586\n",
      "Epoch 2/5\n",
      "106/106 [==============================] - 0s - loss: 0.2891 - val_loss: 0.0235\n",
      "Epoch 3/5\n",
      "106/106 [==============================] - 0s - loss: 0.1978 - val_loss: 0.0848\n",
      "Epoch 4/5\n",
      "106/106 [==============================] - 0s - loss: 0.2576 - val_loss: 0.1200\n",
      "Epoch 5/5\n",
      "106/106 [==============================] - 0s - loss: 0.3052 - val_loss: 0.0995\n",
      "0.0992295247619\n",
      "----------\n",
      "Trying {'activation': 'tanh', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.0001, 'units1': 64, 'units2': 64, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 0.2168 - val_loss: 0.1085\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 0.1949 - val_loss: 0.1063\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.1772 - val_loss: 0.1004\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.1616 - val_loss: 0.0905\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.1455 - val_loss: 0.0819\n",
      "0.0146656399714\n",
      "----------\n",
      "Trying {'activation': 'relu', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.0001, 'units1': 512, 'units2': 512, 'units3': 64, 'window': 30}\n",
      "Train on 277 samples, validate on 15 samples\n",
      "Epoch 1/5\n",
      "277/277 [==============================] - 0s - loss: 0.8093 - val_loss: 0.2019\n",
      "Epoch 2/5\n",
      "277/277 [==============================] - 0s - loss: 0.3283 - val_loss: 0.2150\n",
      "Epoch 3/5\n",
      "277/277 [==============================] - 0s - loss: 0.2425 - val_loss: 0.3138\n",
      "Epoch 4/5\n",
      "277/277 [==============================] - 0s - loss: 0.3066 - val_loss: 0.2392\n",
      "Epoch 5/5\n",
      "277/277 [==============================] - 0s - loss: 0.2916 - val_loss: 0.1200\n",
      "0.0234142774363\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 120}\n",
      "Train on 106 samples, validate on 6 samples\n",
      "Epoch 1/5\n",
      "106/106 [==============================] - 0s - loss: 0.1995 - val_loss: 0.0165\n",
      "Epoch 2/5\n",
      "106/106 [==============================] - 0s - loss: 0.0437 - val_loss: 0.1281\n",
      "Epoch 3/5\n",
      "106/106 [==============================] - 0s - loss: 0.1354 - val_loss: 0.1039\n",
      "Epoch 4/5\n",
      "106/106 [==============================] - 0s - loss: 0.1112 - val_loss: 0.0213\n",
      "Epoch 5/5\n",
      "106/106 [==============================] - 0s - loss: 0.0471 - val_loss: 0.0704\n",
      "0.00544008241834\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function logcosh at 0x1103bbd90>, 'lr': 0.001, 'units1': 512, 'units2': 512, 'units3': 512, 'window': 30}\n",
      "Train on 277 samples, validate on 15 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/277 [==============================] - 0s - loss: 0.4163 - val_loss: 0.4599\n",
      "Epoch 2/5\n",
      "277/277 [==============================] - 0s - loss: 0.4228 - val_loss: 0.2541\n",
      "Epoch 3/5\n",
      "277/277 [==============================] - 0s - loss: 0.2573 - val_loss: 0.2021\n",
      "Epoch 4/5\n",
      "277/277 [==============================] - 0s - loss: 0.1808 - val_loss: 0.0428\n",
      "Epoch 5/5\n",
      "277/277 [==============================] - 0s - loss: 0.0568 - val_loss: 0.1560\n",
      "0.328297387465\n",
      "----------\n",
      "Trying {'activation': 'linear', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.001, 'units1': 512, 'units2': 512, 'units3': 64, 'window': 30}\n",
      "Train on 277 samples, validate on 15 samples\n",
      "Epoch 1/5\n",
      "277/277 [==============================] - 0s - loss: 2.5150 - val_loss: 5.9927\n",
      "Epoch 2/5\n",
      "277/277 [==============================] - 0s - loss: 5.1170 - val_loss: 3.3653\n",
      "Epoch 3/5\n",
      "277/277 [==============================] - 0s - loss: 3.2559 - val_loss: 3.3460\n",
      "Epoch 4/5\n",
      "277/277 [==============================] - 0s - loss: 2.0198 - val_loss: 0.9420\n",
      "Epoch 5/5\n",
      "277/277 [==============================] - 0s - loss: 1.9710 - val_loss: 0.9869\n",
      "1.31609445566\n",
      "----------\n",
      "Trying {'activation': 'linear', 'loss': <function logcosh at 0x1103bbd90>, 'lr': 0.01, 'units1': 512, 'units2': 64, 'units3': 64, 'window': 30}\n",
      "Train on 277 samples, validate on 15 samples\n",
      "Epoch 1/5\n",
      "277/277 [==============================] - 0s - loss: 3.2156 - val_loss: 11.6767\n",
      "Epoch 2/5\n",
      "277/277 [==============================] - 0s - loss: 13.3269 - val_loss: 2.5065\n",
      "Epoch 3/5\n",
      "277/277 [==============================] - 0s - loss: 3.4389 - val_loss: 1.3058\n",
      "Epoch 4/5\n",
      "277/277 [==============================] - 0s - loss: 1.3950 - val_loss: 2.0286\n",
      "Epoch 5/5\n",
      "277/277 [==============================] - 0s - loss: 1.0368 - val_loss: 0.4867\n",
      "1.21311347527\n",
      "----------\n",
      "Trying {'activation': 'linear', 'loss': <function mean_squared_error at 0x1103bb9d8>, 'lr': 0.001, 'units1': 64, 'units2': 512, 'units3': 64, 'window': 30}\n",
      "Train on 277 samples, validate on 15 samples\n",
      "Epoch 1/5\n",
      "277/277 [==============================] - 0s - loss: 1.3157 - val_loss: 0.5743\n",
      "Epoch 2/5\n",
      "277/277 [==============================] - 0s - loss: 0.3929 - val_loss: 1.1745\n",
      "Epoch 3/5\n",
      "277/277 [==============================] - 0s - loss: 2.3112 - val_loss: 0.5906\n",
      "Epoch 4/5\n",
      "277/277 [==============================] - 0s - loss: 0.4205 - val_loss: 1.1402\n",
      "Epoch 5/5\n",
      "277/277 [==============================] - 0s - loss: 1.1304 - val_loss: 0.2929\n",
      "0.312153698027\n",
      "----------\n",
      "Trying {'activation': 'tanh', 'loss': <function mean_absolute_percentage_error at 0x1103bbae8>, 'lr': 0.001, 'units1': 512, 'units2': 64, 'units3': 64, 'window': 180}\n",
      "Something happened\n",
      "----------\n",
      "Trying {'activation': 'linear', 'loss': <function logcosh at 0x1103bbd90>, 'lr': 0.01, 'units1': 512, 'units2': 64, 'units3': 512, 'window': 180}\n",
      "Something happened\n",
      "----------\n",
      "Trying {'activation': 'tanh', 'loss': <function mean_absolute_percentage_error at 0x1103bbae8>, 'lr': 0.001, 'units1': 64, 'units2': 64, 'units3': 64, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 2493804.5000 - val_loss: 1681.0771\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 9656847.0000 - val_loss: 1528.5693\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 8786254.0000 - val_loss: 1678.8021\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 4926971.5000 - val_loss: 2439.8672\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 1326746.8750 - val_loss: 3689.4297\n",
      "0.248679376951\n",
      "----------\n",
      "Trying {'activation': 'relu', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 0.2152 - val_loss: 0.1160\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 0.1255 - val_loss: 0.2179\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.1330 - val_loss: 0.2472\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.1502 - val_loss: 0.2094\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.1266 - val_loss: 0.1400\n",
      "0.0280495224769\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 0.3190 - val_loss: 0.1640\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 0.1683 - val_loss: 0.0290\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.0350 - val_loss: 0.1575\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.1525 - val_loss: 0.1875\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.1826 - val_loss: 0.1540\n",
      "0.0248651875678\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 0.2328 - val_loss: 0.0311\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 0.0438 - val_loss: 0.1704\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.1587 - val_loss: 0.1959\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.1843 - val_loss: 0.1487\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.1374 - val_loss: 0.0606\n",
      "0.00483697720067\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 0.1606 - val_loss: 0.0400\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 0.0452 - val_loss: 0.0703\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.0762 - val_loss: 0.0325\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.0379 - val_loss: 0.0544\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.0539 - val_loss: 0.0586\n",
      "0.00448293867346\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 0.4225 - val_loss: 0.2330\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 0.2242 - val_loss: 0.0394\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.0408 - val_loss: 0.1467\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.1562 - val_loss: 0.2028\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.2120 - val_loss: 0.1936\n",
      "0.0386348911535\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 0.5464 - val_loss: 0.3406\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 0.3463 - val_loss: 0.1403\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.1466 - val_loss: 0.0597\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.0574 - val_loss: 0.1644\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.1583 - val_loss: 0.1898\n",
      "0.0371648467323\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 0.0973 - val_loss: 0.0952\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 0.0972 - val_loss: 0.0814\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.0835 - val_loss: 0.0272\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 0s - loss: 0.0348 - val_loss: 0.0707\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.0709 - val_loss: 0.0747\n",
      "0.00672646200299\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 0.0356 - val_loss: 0.1911\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 0.1907 - val_loss: 0.0784\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.0787 - val_loss: 0.0746\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.0766 - val_loss: 0.1141\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.1143 - val_loss: 0.0872\n",
      "0.00876475626623\n",
      "----------\n",
      "Trying {'activation': 'relu', 'loss': <function mean_squared_error at 0x1103bb9d8>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 0.0438 - val_loss: 0.0190\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 0.0343 - val_loss: 0.0158\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.0266 - val_loss: 0.0119\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.0177 - val_loss: 0.0179\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.0135 - val_loss: 0.0270\n",
      "0.0269239637733\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 0.0385 - val_loss: 0.1757\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 0.1782 - val_loss: 0.0823\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.0851 - val_loss: 0.0575\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.0589 - val_loss: 0.0994\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.0980 - val_loss: 0.0761\n",
      "0.00697141484085\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_squared_error at 0x1103bb9d8>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 0.1389 - val_loss: 0.0310\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 0.0332 - val_loss: 0.0011\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.0019 - val_loss: 0.0217\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.0213 - val_loss: 0.0471\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.0459 - val_loss: 0.0505\n",
      "0.0505474097515\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 0.0359 - val_loss: 0.1930\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 0.1910 - val_loss: 0.0821\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.0806 - val_loss: 0.0699\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.0746 - val_loss: 0.1092\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.1118 - val_loss: 0.0826\n",
      "0.00798315463309\n",
      "----------\n",
      "Trying {'activation': 'relu', 'loss': <function mean_absolute_error at 0x1103bba60>, 'lr': 0.01, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 0.1278 - val_loss: 42.8326\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 34.1835 - val_loss: 3.6134\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 2.9867 - val_loss: 0.6448\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.8452 - val_loss: 0.3951\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.5151 - val_loss: 0.0777\n",
      "0.00773452506156\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_squared_error at 0x1103bb9d8>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 0s - loss: 0.9077 - val_loss: 0.5562\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 0.5656 - val_loss: 0.3001\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.3072 - val_loss: 0.1267\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.1315 - val_loss: 0.0308\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.0334 - val_loss: 0.0015\n",
      "0.00118510802936\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_squared_error at 0x1103bb9d8>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 180}\n",
      "Something happened\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_squared_error at 0x1103bb9d8>, 'lr': 0.01, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 120}\n",
      "Train on 106 samples, validate on 6 samples\n",
      "Epoch 1/5\n",
      "106/106 [==============================] - 0s - loss: 0.0041 - val_loss: 218.0149\n",
      "Epoch 2/5\n",
      "106/106 [==============================] - 0s - loss: 218.0450 - val_loss: 24.6640\n",
      "Epoch 3/5\n",
      "106/106 [==============================] - 0s - loss: 25.1016 - val_loss: 73.1223\n",
      "Epoch 4/5\n",
      "106/106 [==============================] - 0s - loss: 72.0761 - val_loss: 63.2400\n",
      "Epoch 5/5\n",
      "106/106 [==============================] - 0s - loss: 59.4481 - val_loss: 24.3806\n",
      "24.3807066873\n",
      "----------\n",
      "Trying {'activation': 'relu', 'loss': <function mean_squared_error at 0x1103bb9d8>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 1s - loss: 0.1165 - val_loss: 0.0208\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 0.0420 - val_loss: 0.0062\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.0193 - val_loss: 0.0217\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.0261 - val_loss: 0.0439\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.0408 - val_loss: 0.0521\n",
      "0.0536551394508\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_squared_error at 0x1103bb9d8>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 180}\n",
      "Something happened\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function mean_absolute_percentage_error at 0x1103bbae8>, 'lr': 0.01, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 1s - loss: 5557744.5000 - val_loss: 109860.2500\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 190843792.0000 - val_loss: 110525.0000\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 191691184.0000 - val_loss: 98650.1797\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 170880544.0000 - val_loss: 81974.4453\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 141804848.0000 - val_loss: 62829.7148\n",
      "64.4215802569\n",
      "----------\n",
      "Trying {'activation': 'tanh', 'loss': <function mean_squared_error at 0x1103bb9d8>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 120}\n",
      "Train on 106 samples, validate on 6 samples\n",
      "Epoch 1/5\n",
      "106/106 [==============================] - 1s - loss: 0.1173 - val_loss: 0.0220\n",
      "Epoch 2/5\n",
      "106/106 [==============================] - 0s - loss: 0.0545 - val_loss: 0.0727\n",
      "Epoch 3/5\n",
      "106/106 [==============================] - 0s - loss: 0.1056 - val_loss: 0.0427\n",
      "Epoch 4/5\n",
      "106/106 [==============================] - 0s - loss: 0.0686 - val_loss: 0.0106\n",
      "Epoch 5/5\n",
      "106/106 [==============================] - 0s - loss: 0.0312 - val_loss: 0.0169\n",
      "0.0178645453176\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function logcosh at 0x1103bbd90>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 1s - loss: 0.0189 - val_loss: 6.7623e-04\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 9.9922e-04 - val_loss: 0.0104\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.0098 - val_loss: 0.0115\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.0108 - val_loss: 0.0049\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.0047 - val_loss: 7.8655e-04\n",
      "0.0015577548748\n",
      "----------\n",
      "Trying {'activation': 'relu', 'loss': <function logcosh at 0x1103bbd90>, 'lr': 0.0001, 'units1': 512, 'units2': 64, 'units3': 64, 'window': 180}\n",
      "Something happened\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function logcosh at 0x1103bbd90>, 'lr': 0.01, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 120}\n",
      "Train on 106 samples, validate on 6 samples\n",
      "Epoch 1/5\n",
      "106/106 [==============================] - 1s - loss: 0.1628 - val_loss: 13.0320\n",
      "Epoch 2/5\n",
      "106/106 [==============================] - 0s - loss: 13.0310 - val_loss: 11.7359\n",
      "Epoch 3/5\n",
      "106/106 [==============================] - 0s - loss: 11.7379 - val_loss: 8.6952\n",
      "Epoch 4/5\n",
      "106/106 [==============================] - 0s - loss: 8.6998 - val_loss: 4.8963\n",
      "Epoch 5/5\n",
      "106/106 [==============================] - 0s - loss: 4.8986 - val_loss: 1.0649\n",
      "2.9826769576\n",
      "----------\n",
      "Trying {'activation': 'tanh', 'loss': <function logcosh at 0x1103bbd90>, 'lr': 0.001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 1s - loss: 0.0525 - val_loss: 1.7669\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 1.7684 - val_loss: 0.0935\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.2704 - val_loss: 0.6008\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.4096 - val_loss: 0.4977\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.4986 - val_loss: 0.0419\n",
      "0.0923682060536\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function logcosh at 0x1103bbd90>, 'lr': 0.0001, 'units1': 512, 'units2': 64, 'units3': 64, 'window': 30}\n",
      "Train on 277 samples, validate on 15 samples\n",
      "Epoch 1/5\n",
      "277/277 [==============================] - 1s - loss: 0.4656 - val_loss: 0.4310\n",
      "Epoch 2/5\n",
      "277/277 [==============================] - 0s - loss: 0.4272 - val_loss: 0.3917\n",
      "Epoch 3/5\n",
      "277/277 [==============================] - 0s - loss: 0.3899 - val_loss: 0.3545\n",
      "Epoch 4/5\n",
      "277/277 [==============================] - 0s - loss: 0.3544 - val_loss: 0.3183\n",
      "Epoch 5/5\n",
      "277/277 [==============================] - 0s - loss: 0.3198 - val_loss: 0.2834\n",
      "0.623194283215\n",
      "----------\n",
      "Trying {'activation': 'linear', 'loss': <function logcosh at 0x1103bbd90>, 'lr': 0.0001, 'units1': 64, 'units2': 512, 'units3': 512, 'window': 120}\n",
      "Train on 106 samples, validate on 6 samples\n",
      "Epoch 1/5\n",
      "106/106 [==============================] - 1s - loss: 0.3984 - val_loss: 0.1573\n",
      "Epoch 2/5\n",
      "106/106 [==============================] - 0s - loss: 0.0932 - val_loss: 0.4515\n",
      "Epoch 3/5\n",
      "106/106 [==============================] - 0s - loss: 0.2316 - val_loss: 0.3907\n",
      "Epoch 4/5\n",
      "106/106 [==============================] - 0s - loss: 0.2133 - val_loss: 0.1880\n",
      "Epoch 5/5\n",
      "106/106 [==============================] - 0s - loss: 0.1027 - val_loss: 0.0343\n",
      "0.0698897526355\n",
      "----------\n",
      "Trying {'activation': 'sigmoid', 'loss': <function logcosh at 0x1103bbd90>, 'lr': 0.001, 'units1': 512, 'units2': 512, 'units3': 512, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 1s - loss: 0.0698 - val_loss: 1.3011\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 1.2871 - val_loss: 0.3636\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.3604 - val_loss: 0.1399\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.1373 - val_loss: 0.5390\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.5309 - val_loss: 0.4207\n",
      "0.966313394617\n",
      "----------\n",
      "Trying {'activation': 'tanh', 'loss': <function mean_absolute_percentage_error at 0x1103bbae8>, 'lr': 0.01, 'units1': 64, 'units2': 64, 'units3': 64, 'window': 30}\n",
      "Train on 277 samples, validate on 15 samples\n",
      "Epoch 1/5\n",
      "277/277 [==============================] - 1s - loss: 7969821.6257 - val_loss: 20186.4375\n",
      "Epoch 2/5\n",
      "277/277 [==============================] - 0s - loss: 40982487.4820 - val_loss: 4230.4878\n",
      "Epoch 3/5\n",
      "277/277 [==============================] - 0s - loss: 13405830.1227 - val_loss: 5687.4067\n",
      "Epoch 4/5\n",
      "277/277 [==============================] - 0s - loss: 16480876.7068 - val_loss: 13583.9170\n",
      "Epoch 5/5\n",
      "277/277 [==============================] - 0s - loss: 23714918.4116 - val_loss: 5559.7456\n",
      "2.29716804867\n",
      "----------\n",
      "Trying {'activation': 'linear', 'loss': <function mean_squared_error at 0x1103bb9d8>, 'lr': 0.0001, 'units1': 512, 'units2': 512, 'units3': 512, 'window': 180}\n",
      "Something happened\n",
      "----------\n",
      "Trying {'activation': 'relu', 'loss': <function logcosh at 0x1103bbd90>, 'lr': 0.001, 'units1': 64, 'units2': 64, 'units3': 64, 'window': 60}\n",
      "Train on 220 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "220/220 [==============================] - 1s - loss: 0.1862 - val_loss: 0.0304\n",
      "Epoch 2/5\n",
      "220/220 [==============================] - 0s - loss: 0.0806 - val_loss: 0.0423\n",
      "Epoch 3/5\n",
      "220/220 [==============================] - 0s - loss: 0.0776 - val_loss: 0.0300\n",
      "Epoch 4/5\n",
      "220/220 [==============================] - 0s - loss: 0.0392 - val_loss: 0.0397\n",
      "Epoch 5/5\n",
      "220/220 [==============================] - 0s - loss: 0.0208 - val_loss: 0.0636\n",
      "0.129514122711\n",
      "----------\n",
      "best: \n",
      "{'activation': 1, 'loss': 1, 'lr': 2, 'units1': 0, 'units2': 1, 'units3': 1, 'window': 1}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(experiment, space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "print ('best: ')\n",
    "print (best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
