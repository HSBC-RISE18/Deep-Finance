{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D \n",
    "from keras.layers.core import Flatten, Lambda\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Merge\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import SpatialDropout2D\n",
    "from keras.layers import Dropout, Reshape\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "# local library\n",
    "from rlearn import memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    \"\"\"Deep Q-Learning Network\n",
    "    \n",
    "    Basend on DQN and Multiscale CNN, find the optimal time to \n",
    "    exit from a stock market.\n",
    "    \n",
    "    Available function\n",
    "    - build_model: build network based on tensorflow and keras\n",
    "    - train: given DateFrame stock data, train network\n",
    "    - predict_action: givne DataFrame stock data, return optimal protfolio\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"initialized approximate value function\n",
    "        \n",
    "        config should have the following attributes\n",
    "        \n",
    "        Args:\n",
    "            device: the device to use computation, e.g. '/gpu:0'\n",
    "            gamma(float): the decay rate for value at RL\n",
    "            history_length(int): input_length for each scale at CNN\n",
    "            n_feature(int): the number of type of input \n",
    "                (e.g. the number of company to use at stock trading)\n",
    "            n_history(int): the nubmer of history that will be used as input\n",
    "            n_smooth, n_down(int): the number of smoothed and down sampling input at CNN\n",
    "            k_w(int): the size of filter at CNN\n",
    "            n_hidden(int): the size of fully connected layer\n",
    "            n_batch(int): the size of mini batch\n",
    "            n_epochs(int): the training epoch for each time\n",
    "            update_rate (0, 1): parameter for soft update\n",
    "            learning_rate(float): learning rate for SGD\n",
    "            memory_length(int): the length of Replay Memory\n",
    "            n_memory(int): the number of different Replay Memories\n",
    "            alpha, beta: [0, 1] parameters for Prioritized Replay Memories\n",
    "        \"\"\"\n",
    "        self.device = config.device\n",
    "        self.save_path = config.save_path\n",
    "        self.is_load = config.is_load\n",
    "        self.gamma = config.gamma\n",
    "        self.history_length = config.history_length\n",
    "        self.n_stock = config.n_stock\n",
    "        self.n_feature = config.n_feature\n",
    "        self.n_smooth = config.n_smooth\n",
    "        self.n_down = config.n_down\n",
    "        self.k_w = config.k_w\n",
    "        self.n_hidden = config.n_hidden\n",
    "        self.n_batch = config.n_batch\n",
    "        self.n_epochs = config.n_epochs\n",
    "        self.update_rate = config.update_rate\n",
    "        self.alpha = config.alpha\n",
    "        self.beta = config.beta\n",
    "        self.lr = config.learning_rate\n",
    "        self.memory_length = config.memory_length\n",
    "        self.n_memory = config.n_memory\n",
    "        # the length of the data as input\n",
    "        self.n_history = max(self.n_smooth + self.history_length, (self.n_down + 1) * self.history_length)\n",
    "        print (\"building model....\")\n",
    "        # have compatibility with new tensorflow\n",
    "        tf.python.control_flow_ops = tf\n",
    "        # avoid creating _LEARNING_PHASE outside the network\n",
    "        K.clear_session()\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "        K.set_session(self.sess)\n",
    "        with self.sess.as_default():\n",
    "            with tf.device(self.device):\n",
    "                self.build_model()\n",
    "        print('finished building model!')\n",
    "    \n",
    "    def train(self, input_data, noise_scale=0.1):\n",
    "        \"\"\"training DQN, which has two actions: 0-exit, 1-stay\n",
    "        \n",
    "        Args:\n",
    "            data (DataFrame): stock price for self.n_feature companies\n",
    "        \"\"\"\n",
    "        stock_data = input_data.values\n",
    "        date = input_data.index\n",
    "        T = len(stock_data)\n",
    "        self.noise_scale = noise_scale\n",
    "        \n",
    "        # frequency for output\n",
    "        print_freq = int(T / 100)\n",
    "        if print_freq == 0:\n",
    "            print_freq = 1\n",
    "        print (\"training....\")\n",
    "        st = time.time()\n",
    "        #  udpate rate for prioritizing parameter\n",
    "        db = (1 - self.beta) / 1000\n",
    "        \n",
    "        # result for return value\n",
    "        values = [[] for _ in range(self.n_stock)]\n",
    "        date_label = [[] for _ in range(self.n_stock)]\n",
    "        date_use = []\n",
    "        stock_use = []\n",
    "        # will not train until getting enough data\n",
    "        t0 = self.n_history + self.n_batch\n",
    "        self.initialize_memory(stock_data[:t0], scale=noise_scale)\n",
    "        save_data_freq = 10\n",
    "        save_weight_freq = 10\n",
    "        count = 0\n",
    "        input_data.to_csv(\"stock_price.csv\")\n",
    "        for t in range(t0, T):\n",
    "            stock_use.append(stock_data[t])\n",
    "            date_use.append(date[t])\n",
    "            action = self.predict_action(stock_data[t])\n",
    "            for i in range(self.n_stock):\n",
    "                if action[i] == 0:\n",
    "                    date_label[i].append(date[t])\n",
    "                    values[i].append(stock_data[t][i])\n",
    "            self.update_memory(stock_data[t])\n",
    "            count += 1\n",
    "            for epoch in range(self.n_epochs):    \n",
    "                # select transition from pool\n",
    "                self.update_weight()\n",
    "                # update prioritizing paramter untill it goes over 1\n",
    "            self.beta  += db\n",
    "            if self.beta >= 1.0:\n",
    "                self.beta = 1.0\n",
    "            idx = np.random.randint(0, self.n_memory)\n",
    "            \n",
    "            experiences, weights = self.memory[idx].sample(self.n_batch, self.n_history, self.alpha, self.beta)\n",
    "            max_idx = self.get_max_idx(experiences.state1)\n",
    "            target_value = self.sess.run(self.target_value,\n",
    "                                     feed_dict={self.state_target: experiences.state1,\n",
    "                                 self.reward: experiences.reward,\n",
    "                                               self.max_idx_target: max_idx})\n",
    "            \n",
    "            if t % print_freq == 0:\n",
    "                print (\"time:\",  date[t])\n",
    "                error = self.sess.run(self.error,\n",
    "                              feed_dict={self.state: experiences.state0,\n",
    "                                         self.target: target_value,\n",
    "                                         self.reward: experiences.reward,\n",
    "                                         K.learning_phase(): 0})\n",
    "                print(\"error:\", np.mean(error))\n",
    "                action = self.predict_action(stock_data[t])\n",
    "                print(\"portfolio:\", action)\n",
    "                print (\"elapsed time\", time.time() - st)\n",
    "                print(\"********************************************************************\")\n",
    "                \n",
    "            if count % save_data_freq == 0:\n",
    "                for i in range(self.n_stock):\n",
    "                    result = pd.DataFrame(values[i], index=pd.DatetimeIndex(date_label[i]))\n",
    "                    result.to_csv(\"exit_result_{}.csv\".format(i))\n",
    "                data_use = pd.DataFrame(stock_use, index=pd.DatetimeIndex(date_use))\n",
    "                data_use.to_csv(\"stock_price.csv\")\n",
    "                \n",
    "            if count % save_weight_freq == 0:\n",
    "                save_path = self.saver.save(self.sess, self.save_path)\n",
    "                print(\"Model saved in file: %s\" % self.save_path)\n",
    "\n",
    "        save_path = self.saver.save(self.sess, self.save_path)\n",
    "        print(\"Model saved in file: %s\" % self.save_path)\n",
    "        print (\"finished training\")\n",
    "        \n",
    "        return [pd.DataFrame(values[i], index=pd.DatetimeIndex(date_label[i])) for i in range(self.n_stock)]\n",
    "    \n",
    "    def predict_action(self, state):\n",
    "        \"\"\"Preduct Optimal strategy\n",
    "        \n",
    "        Args:\n",
    "            state(float): stock data with size: [self.n_stock, ]\n",
    "        Retrun:\n",
    "            integer: 0-exit, 1-stay\n",
    "        \"\"\"\n",
    "        pred_state = self.memory[0].sample_state_uniform(self.n_batch, self.n_history)\n",
    "        new_state = pred_state[-1]\n",
    "        new_state = np.concatenate((new_state[1:], [state]), axis=0)\n",
    "        pred_state = np.concatenate((pred_state[:-1], [new_state]), axis=0)\n",
    "        action = self.max_action.eval(\n",
    "            session=self.sess,\n",
    "            feed_dict={self.state: pred_state, K.learning_phase(): 0})[-1]\n",
    "        return action\n",
    "    \n",
    "    def update_weight(self):\n",
    "        \"\"\"Update networks' parameters and memories\"\"\"\n",
    "        idx = np.random.randint(0, self.n_memory)\n",
    "        experiences, weights = self.memory[idx].sample(self.n_batch, self.n_history, self.alpha, self.beta)\n",
    "        max_idx = self.get_max_idx(experiences.state1)\n",
    "        # get target value for optimization\n",
    "        target_value = self.sess.run(self.target_value,\n",
    "                                     feed_dict={self.state_target: experiences.state1,\n",
    "                                 self.reward: experiences.reward,\n",
    "                                               self.max_idx_target: max_idx})\n",
    "        # optimize network\n",
    "        self.sess.run(self.critic_optim, \n",
    "                      feed_dict={self.state: experiences.state0,\n",
    "                                 self.target: target_value,\n",
    "                                 self.weights: weights,\n",
    "                                 self.learning_rate: self.lr,\n",
    "                                 K.learning_phase(): 1})  \n",
    "        # compute errors to determine prioritizing ratio\n",
    "        error = self.sess.run(self.error,\n",
    "                              feed_dict={self.state: experiences.state0,\n",
    "                                         self.target: target_value,\n",
    "                                         self.reward: experiences.reward,\n",
    "                                         K.learning_phase(): 0})\n",
    "        self.memory[idx].update_priority(error)\n",
    "        # softupdate for critic network\n",
    "        old_weights = self.critic_target.get_weights()\n",
    "        new_weights = self.critic.get_weights()\n",
    "        weights = [self.update_rate * new_w + (1 - self.update_rate) * old_w\n",
    "                   for new_w, old_w in zip(new_weights, old_weights)]\n",
    "        self.critic_target.set_weights(weights)\n",
    "        \n",
    "    def initialize_memory(self, stocks, scale=10):\n",
    "        self.memory = []\n",
    "        for i in range(self.n_memory):\n",
    "            self.memory.append(memory.SequentialMemory(self.memory_length))\n",
    "        for t in range(len(stocks)):\n",
    "            for idx_memory in range(self.n_memory):\n",
    "                action = None\n",
    "                reward = np.concatenate((np.reshape(stocks[t], (self.n_stock, 1)), np.zeros((self.n_stock, 1))), axis=-1)\n",
    "                self.memory[idx_memory].append(stocks[t], action, reward)\n",
    "        \n",
    "    def update_memory(self, state):\n",
    "        \"\"\"Update memory without updating weight\"\"\"\n",
    "        for i in range(self.n_memory):\n",
    "            self.memory[i].observations.append(state)\n",
    "            self.memory[i].priority.append(1.0)\n",
    "        # to stabilize batch normalization, use other samples for prediction\n",
    "        pred_state = self.memory[0].sample_state_uniform(self.n_batch, self.n_history)\n",
    "        for i in range(self.n_memory):\n",
    "            action_off = None\n",
    "            reward_off = np.concatenate((np.reshape(state, (self.n_stock, 1)), np.zeros((self.n_stock, 1))), axis=-1)\n",
    "            self.memory[i].rewards.append(reward_off)\n",
    "            self.memory[i].actions.append(action_off)\n",
    "    \n",
    "    def get_max_idx(self, state):\n",
    "        max_action = self.sess.run(self.max_action_target, feed_dict={self.state_target: state})\n",
    "        shape = max_action.shape\n",
    "        max_idx = []\n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "                max_idx.append([i, j, max_action[i][j]])\n",
    "        return np.array(max_idx, dtype=int)\n",
    "    \n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build all of the network and optimizations\n",
    "        \n",
    "        just for conveninece of trainig, seprate placehoder for train and target network\n",
    "        critic network input: [raw_data, smoothed, downsampled]\n",
    "        \"\"\"\n",
    "        self.critic = self.build_critic()\n",
    "        self.critic_target = self.build_critic()\n",
    "        # transform input into the several scales and smoothing\n",
    "        self.state =  tf.placeholder(tf.float32, [None, self.n_history, self.n_stock], name='state')\n",
    "        self.state_target = tf.placeholder(tf.float32, [None, self.n_history, self.n_stock], name='state_target')\n",
    "        # reshape to convolutional input\n",
    "        state_ = tf.reshape(self.state, [-1, self.n_history, self.n_stock, 1])\n",
    "        state_target_ = tf.reshape(self.state_target, [-1, self.n_history, self.n_stock, 1])\n",
    "        raw, smoothed, down = self.transform_input(state_)\n",
    "        raw_target, smoothed_target, down_target = self.transform_input(state_target_)\n",
    "        \n",
    "        # build graph for citic training\n",
    "        input_q = [raw,] +  smoothed + down\n",
    "        self.Q = self.critic(input_q)\n",
    "        self.max_action = tf.argmax(self.Q, dimension=2)\n",
    "        # target network\n",
    "        input_q_target = [raw_target,] +  smoothed_target + down_target\n",
    "        Q_target = self.critic_target(input_q_target)\n",
    "        self.reward = tf.placeholder(tf.float32, [None, self.n_stock, 2], name='reward')\n",
    "        double_Q = self.critic(input_q_target)\n",
    "        self.max_action_target = tf.argmax(double_Q, 2)\n",
    "        self.max_idx_target = tf.placeholder(tf.int32, [None, 3], \"double_idx\")\n",
    "        Q_max = tf.gather_nd(Q_target, self.max_idx_target)\n",
    "        Q_max = tf.reshape(Q_max, [-1, self.n_stock, 1])\n",
    "        Q_value = tf.concat(2, (tf.zeros_like(Q_max), Q_max))\n",
    "        self.target_value = self.reward  + self.gamma * Q_value\n",
    "        self.target_value = tf.cast(self.target_value, tf.float32)\n",
    "        self.target = tf.placeholder(tf.float32, [None, self.n_stock, 2], name=\"target_value\")\n",
    "        # optimization\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[], name=\"learning_rate\")\n",
    "        # get rid of bias of prioritized\n",
    "        self.weights = tf.placeholder(tf.float32, shape=[None], name=\"weights\")\n",
    "        self.loss = tf.reduce_mean(self.weights * tf.reduce_sum(tf.square(self.target - self.Q), [1, 2]), name='loss')\n",
    "        # TD-error for priority\n",
    "        self.error = tf.reduce_sum(tf.abs(self.target - self.Q), [1, 2])\n",
    "        self.critic_optim = tf.train.AdamOptimizer(self.learning_rate) \\\n",
    "            .minimize(self.loss, var_list=self.critic.trainable_weights)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        is_initialize = True\n",
    "        if self.is_load:\n",
    "            if self.load(self.save_path):\n",
    "                print('succeded to load')\n",
    "                is_initialize = False\n",
    "            else:\n",
    "                print('failed to load')\n",
    "        \n",
    "        # initialize network\n",
    "        tf.initialize_all_variables().run(session=self.sess)\n",
    "        weights = self.critic.get_weights()\n",
    "        self.critic_target.set_weights(weights)\n",
    "        \n",
    "    def build_critic(self):\n",
    "        \"\"\"Build critic network\n",
    "        \n",
    "        recieve transformed tensor: raw_data, smooted_data, and downsampled_data\n",
    "        \"\"\"\n",
    "        nf = self.n_feature\n",
    "        # layer1\n",
    "        # smoothed input\n",
    "        sm_model = [Sequential() for _ in range(self.n_smooth)]\n",
    "        for m in sm_model:\n",
    "            m.add(Lambda(lambda x: x,  input_shape=(self.history_length, self.n_stock, 1)))\n",
    "            m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "            m.add(BatchNormalization(mode=2, axis=-1))\n",
    "            m.add(PReLU())\n",
    "        # down sampled input\n",
    "        dw_model = [Sequential() for _ in range(self.n_down)]\n",
    "        for m in dw_model:\n",
    "            m.add(Lambda(lambda x: x,  input_shape=(self.history_length, self.n_stock, 1)))\n",
    "            m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "            m.add(BatchNormalization(mode=2, axis=-1))\n",
    "            m.add(PReLU())\n",
    "        # raw input\n",
    "        state = Sequential()\n",
    "        nf = self.n_feature\n",
    "        state.add(Lambda(lambda x: x,  input_shape=(self.history_length, self.n_stock, 1)))\n",
    "        state.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "        state.add(BatchNormalization(mode=2, axis=-1))\n",
    "        state.add(PReLU())\n",
    "        merged = Merge([state,] + sm_model + dw_model, mode='concat', concat_axis=-1)\n",
    "        # layer2\n",
    "        nf = nf * 2\n",
    "        model = Sequential()\n",
    "        model.add(merged)\n",
    "        model.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "        model.add(BatchNormalization(mode=2, axis=-1))\n",
    "        model.add(PReLU())\n",
    "        model.add(Flatten())\n",
    "        # layer3\n",
    "        model.add(Dense(self.n_hidden))\n",
    "        model.add(BatchNormalization(mode=1, axis=-1))\n",
    "        model.add(PReLU())\n",
    "        # layer4\n",
    "        model.add(Dense(int(np.sqrt(self.n_hidden))))\n",
    "        model.add(PReLU())\n",
    "        # output\n",
    "        model.add(Dense(2 * self.n_stock))\n",
    "        model.add(Reshape((self.n_stock, 2)))\n",
    "        return model\n",
    "    \n",
    "    def transform_input(self, input):\n",
    "        \"\"\"Transform data into the Multi Scaled one\n",
    "        \n",
    "        Args:\n",
    "            input: tensor with shape: [None, self.n_history, self.n_stock]\n",
    "        Return:\n",
    "            list of the same shape tensors, [None, self.length_history, self.n_stock]\n",
    "        \"\"\"\n",
    "        # the last data is the newest information\n",
    "        raw = input[:, self.n_history - self.history_length:, :, :]\n",
    "        # smooth data\n",
    "        smoothed = []\n",
    "        for n_sm in range(2, self.n_smooth + 2):\n",
    "            smoothed.append(\n",
    "                tf.reduce_mean(tf.pack([input[:, self.n_history - st - self.history_length:self.n_history - st, :, :]\n",
    "                                        for st in range(n_sm)]),0))\n",
    "        # downsample data\n",
    "        down = []\n",
    "        for n_dw in range(2, self.n_down + 2):\n",
    "            sampled_ = tf.pack([input[:, idx, :, :] \n",
    "                                for idx in range(self.n_history-n_dw*self.history_length, self.n_history, n_dw)])\n",
    "            down.append(tf.transpose(sampled_, [1, 0, 2, 3]))\n",
    "        return raw, smoothed, down\n",
    "    \n",
    "    def load(self, checkpoint_dir):\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        try:\n",
    "            self.saver.restore(self.sess, self.save_path)\n",
    "            return True\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNConfig(object):\n",
    "    def __init__(self, n_stock):\n",
    "        self.device = '/gpu:0'\n",
    "        self.save_path = '/path/to/your/save/path/model.ckpt'\n",
    "        self.is_load = False\n",
    "        self.gamma = 0.999\n",
    "        self.history_length = 10\n",
    "        self.n_stock = n_stock\n",
    "        self.n_smooth = 5\n",
    "        self.n_down = 5\n",
    "        self.k_w = 3\n",
    "        self.n_hidden = 100\n",
    "        self.n_batch = 32\n",
    "        self.n_epochs = 100\n",
    "        self.n_feature = 32\n",
    "        self.alpha = 0.7\n",
    "        self.beta = 0.5\n",
    "        self.update_rate = 1e-1\n",
    "        self.learning_rate = 1e-3\n",
    "    \n",
    "        # memory_config\n",
    "        self.memory_length = 200\n",
    "        self.n_memory = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(arg):\n",
    "    st = time.time()\n",
    "    #symbols = utils.get_sap_symbols('sap500')\n",
    "    #np.random.shuffle(symbols)\n",
    "    #chosen_symbols = symbols[:10]\n",
    "    #start_date=\"2009-04-01\"\n",
    "    #end_date=\"2015-03-31\"\n",
    "    # use Open data\n",
    "    input_data = reindexed_data\n",
    "    elapsed = time.time() - st\n",
    "    print (\"time for getting data:\", elapsed)\n",
    "\n",
    "    train_st = (\"2015-01-12\")\n",
    "    train_end = (\"2017-01-12\")\n",
    "    test_st = (\"2017-01-13\")\n",
    "    test_end = (\"2018-01-10\")\n",
    "\n",
    "    train_input = input_data.loc[(input_data.index >= train_st) & (input_data.index <= train_end)]\n",
    "    test_input = input_data.loc[(input_data.index >= test_st) & (input_data.index <= test_end)]\n",
    "    \n",
    "    # training\n",
    "    n_stock = len(train_input.values[0])\n",
    "    sys.path.append(\"./model\")\n",
    "    print(arg)\n",
    "\n",
    "\n",
    "    if arg == \"ddpg\":\n",
    "        from ddpg import DDPG\n",
    "        from config import DDPGConfig\n",
    "        config = DDPGConfig(n_stock)\n",
    "        ddpg = DDPG(config)\n",
    "        values = ddpg.train(train_input)\n",
    "    elif arg == \"dqn\":\n",
    "        from dqn import DQN\n",
    "        from config import DQNConfig\n",
    "        config = DQNConfig(n_stock)\n",
    "        dqn = DQN(config)\n",
    "        values = dqn.train(train_input)\n",
    "        return values\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # prediction\n",
    "    profit = []\n",
    "    date = []\n",
    "    index = test_input.index\n",
    "    values = test_input.values\n",
    "    old_value = values[0]\n",
    "    prof = 0\n",
    "    count = 0\n",
    "    for i in range(1, len(index)):\n",
    "        value = values[i]\n",
    "        action = ddpg.predict_action(old_value)\n",
    "        ddpg.update_memory(old_value, value)\n",
    "        gain = np.sum((value - old_value) * action)\n",
    "        prof += gain\n",
    "        profit.append(prof)\n",
    "        date.append(index[i])\n",
    "        if count%10 == 0:\n",
    "            result = pd.DataFrame(profit, index=pd.DatetimeIndex(date))\n",
    "            result.to_csv(\"test_result.csv\")\n",
    "        count += 1\n",
    "        if count%10 == 0:\n",
    "            print('time:', index[i])\n",
    "            print('portfolio:', action)\n",
    "            print('profit:', prof)\n",
    "        print('***************************')\n",
    "        for i in range(100):\n",
    "            ddpg.update_weight()\n",
    "        old_value = value\n",
    "    result = pd.DataFrame(profit, index=pd.DatetimeIndex(date))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for getting data: 0.0\n",
      "-f\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    arg = sys.argv[1]\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    result = main(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_csv('dataset/700_data_from_IB_1day_3year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>barCount</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-12</td>\n",
       "      <td>128.7</td>\n",
       "      <td>128.8</td>\n",
       "      <td>126.2</td>\n",
       "      <td>127.1</td>\n",
       "      <td>15618900</td>\n",
       "      <td>7567</td>\n",
       "      <td>127.55145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-13</td>\n",
       "      <td>125.6</td>\n",
       "      <td>128.3</td>\n",
       "      <td>125.2</td>\n",
       "      <td>127.8</td>\n",
       "      <td>15281600</td>\n",
       "      <td>6962</td>\n",
       "      <td>126.59100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-14</td>\n",
       "      <td>126.7</td>\n",
       "      <td>127.7</td>\n",
       "      <td>125.1</td>\n",
       "      <td>126.0</td>\n",
       "      <td>15826600</td>\n",
       "      <td>6479</td>\n",
       "      <td>126.19990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>125.5</td>\n",
       "      <td>127.0</td>\n",
       "      <td>124.1</td>\n",
       "      <td>126.9</td>\n",
       "      <td>14878200</td>\n",
       "      <td>6298</td>\n",
       "      <td>125.29615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-01-16</td>\n",
       "      <td>125.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>121.6</td>\n",
       "      <td>121.9</td>\n",
       "      <td>23635900</td>\n",
       "      <td>8621</td>\n",
       "      <td>122.85570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        date   open   high    low  close    volume  barCount  \\\n",
       "0           0  2015-01-12  128.7  128.8  126.2  127.1  15618900      7567   \n",
       "1           1  2015-01-13  125.6  128.3  125.2  127.8  15281600      6962   \n",
       "2           2  2015-01-14  126.7  127.7  125.1  126.0  15826600      6479   \n",
       "3           3  2015-01-15  125.5  127.0  124.1  126.9  14878200      6298   \n",
       "4           4  2015-01-16  125.0  125.0  121.6  121.9  23635900      8621   \n",
       "\n",
       "     average  \n",
       "0  127.55145  \n",
       "1  126.59100  \n",
       "2  126.19990  \n",
       "3  125.29615  \n",
       "4  122.85570  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adjusted_data = input_data.drop(['Unnamed: 0'], axis = 1)\n",
    "reindexed_data = adjusted_data.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>barCount</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-12</th>\n",
       "      <td>128.7</td>\n",
       "      <td>128.8</td>\n",
       "      <td>126.2</td>\n",
       "      <td>127.1</td>\n",
       "      <td>15618900</td>\n",
       "      <td>7567</td>\n",
       "      <td>127.55145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-13</th>\n",
       "      <td>125.6</td>\n",
       "      <td>128.3</td>\n",
       "      <td>125.2</td>\n",
       "      <td>127.8</td>\n",
       "      <td>15281600</td>\n",
       "      <td>6962</td>\n",
       "      <td>126.59100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-14</th>\n",
       "      <td>126.7</td>\n",
       "      <td>127.7</td>\n",
       "      <td>125.1</td>\n",
       "      <td>126.0</td>\n",
       "      <td>15826600</td>\n",
       "      <td>6479</td>\n",
       "      <td>126.19990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-15</th>\n",
       "      <td>125.5</td>\n",
       "      <td>127.0</td>\n",
       "      <td>124.1</td>\n",
       "      <td>126.9</td>\n",
       "      <td>14878200</td>\n",
       "      <td>6298</td>\n",
       "      <td>125.29615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-16</th>\n",
       "      <td>125.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>121.6</td>\n",
       "      <td>121.9</td>\n",
       "      <td>23635900</td>\n",
       "      <td>8621</td>\n",
       "      <td>122.85570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             open   high    low  close    volume  barCount    average\n",
       "date                                                                 \n",
       "2015-01-12  128.7  128.8  126.2  127.1  15618900      7567  127.55145\n",
       "2015-01-13  125.6  128.3  125.2  127.8  15281600      6962  126.59100\n",
       "2015-01-14  126.7  127.7  125.1  126.0  15826600      6479  126.19990\n",
       "2015-01-15  125.5  127.0  124.1  126.9  14878200      6298  125.29615\n",
       "2015-01-16  125.0  125.0  121.6  121.9  23635900      8621  122.85570"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reindexed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>barCount</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>426.8</td>\n",
       "      <td>432.400</td>\n",
       "      <td>424.2</td>\n",
       "      <td>431.8</td>\n",
       "      <td>25832100</td>\n",
       "      <td>38798</td>\n",
       "      <td>429.03195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>436.4</td>\n",
       "      <td>436.400</td>\n",
       "      <td>428.2</td>\n",
       "      <td>433.2</td>\n",
       "      <td>18080400</td>\n",
       "      <td>33014</td>\n",
       "      <td>431.49310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>436.2</td>\n",
       "      <td>439.200</td>\n",
       "      <td>433.8</td>\n",
       "      <td>438.6</td>\n",
       "      <td>16035700</td>\n",
       "      <td>24577</td>\n",
       "      <td>436.72370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>440.8</td>\n",
       "      <td>446.400</td>\n",
       "      <td>439.0</td>\n",
       "      <td>443.8</td>\n",
       "      <td>22790600</td>\n",
       "      <td>34282</td>\n",
       "      <td>443.34265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>445.6</td>\n",
       "      <td>446.201</td>\n",
       "      <td>440.0</td>\n",
       "      <td>440.4</td>\n",
       "      <td>16907900</td>\n",
       "      <td>28656</td>\n",
       "      <td>442.37575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             open     high    low  close    volume  barCount    average\n",
       "date                                                                   \n",
       "2018-01-04  426.8  432.400  424.2  431.8  25832100     38798  429.03195\n",
       "2018-01-05  436.4  436.400  428.2  433.2  18080400     33014  431.49310\n",
       "2018-01-08  436.2  439.200  433.8  438.6  16035700     24577  436.72370\n",
       "2018-01-09  440.8  446.400  439.0  443.8  22790600     34282  443.34265\n",
       "2018-01-10  445.6  446.201  440.0  440.4  16907900     28656  442.37575"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reindexed_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
