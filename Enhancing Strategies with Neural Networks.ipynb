{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Strategies with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: Alex Honchar\n",
    "\n",
    "Source: https://github.com/Rachnog/Deep-Trading/blob/master/volatility/volatility.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Permute, Reshape, Lambda\n",
    "from keras.layers import Merge, Input, concatenate\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, RepeatVector, AveragePooling1D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.optimizers import RMSprop, Adam, SGD, Nadam\n",
    "from keras.initializers import *\n",
    "from keras.constraints import *\n",
    "from keras import regularizers\n",
    "from keras import losses\n",
    "from keras.layers.noise import *\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "import seaborn as sns\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyti.williams_percent_r import williams_percent_r\n",
    "from pyti.relative_strength_index import relative_strength_index\n",
    "\n",
    "import nolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data2change(data):\n",
    "    change = pd.DataFrame(data).pct_change()\n",
    "    change = change.replace([np.inf, -np.inf], np.nan)\n",
    "    change = change.fillna(0.).values.tolist()\n",
    "    change = [c[0] for c in change]\n",
    "    return change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remap(x, in_min, in_max, out_min, out_max):\n",
    "  return (x - in_min) * (out_max - out_min) / (in_max - in_min) + out_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moving_average_convergence(group, nslow=12, nfast=6):\n",
    "    emaslow = pd.ewma(group, span=nslow, min_periods=1).values.tolist()\n",
    "    emafast = pd.ewma(group, span=nfast, min_periods=1).values.tolist()\n",
    "    return np.array(emafast) -np.array(emaslow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    # courtsey http://stackoverflow.com/users/190280/josh-bleecher-snyder\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_Xt_Yt(X, y, percentage=0.95):\n",
    "    p = int(len(X) * percentage)\n",
    "    X_train = X[0:p]\n",
    "    Y_train = y[0:p]\n",
    "     \n",
    "    X_train, Y_train = shuffle_in_unison(X_train, Y_train)\n",
    " \n",
    "    X_test = X[p:]\n",
    "    Y_test = y[p:]\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_SET = 0.9\n",
    "WINDOW = 30\n",
    "STEP = 1\n",
    "FORECAST = 14\n",
    "ROLLING = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_original = pd.read_csv('dataset/700_data_from_IB_1day.csv')[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAewAAAFVCAYAAAAt79zdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSimllAPoga2UUko5gB7YSimllAP8BW5+TCw0VYQOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125056f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data_original['close'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "openp = data_original.loc[:, 'open'].tolist()\n",
    "highp = data_original.loc[:, 'high'].tolist()\n",
    "lowp = data_original.loc[:, 'low'].tolist()\n",
    "closep = data_original.loc[:, 'close'].tolist()\n",
    "volumep = data_original.loc[:, 'volume'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ma30 = pd.DataFrame(closep).rolling(14).mean().values.tolist()\n",
    "ma30 = [v[0] for v in ma30]\n",
    "ma60 = pd.DataFrame(closep).rolling(30).mean().values.tolist()\n",
    "ma60 = [v[0] for v in ma60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nine_period_high = pd.rolling_max(pd.DataFrame(highp), window= int(ROLLING / 2))\n",
    "nine_period_high = pd.DataFrame(highp).rolling(window = int(ROLLING / 2)).max()\n",
    "#nine_period_low = pd.rolling_min(pd.DataFrame(lowp), window=  int(ROLLING / 2))\n",
    "nine_period_low = pd.DataFrame(lowp).rolling(window = int(ROLLING /2)).min()\n",
    "ichimoku = (nine_period_high + nine_period_low) /2\n",
    "ichimoku = ichimoku.replace([np.inf, -np.inf], np.nan)\n",
    "ichimoku = ichimoku.fillna(0.).values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hussainmohammadyousuf/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: pd.ewm_mean is deprecated for DataFrame and will be removed in a future version, replace with \n",
      "\tDataFrame.ewm(span=12,min_periods=1,adjust=True,ignore_na=False).mean()\n",
      "  \n",
      "/Users/hussainmohammadyousuf/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: pd.ewm_mean is deprecated for DataFrame and will be removed in a future version, replace with \n",
      "\tDataFrame.ewm(span=6,min_periods=1,adjust=True,ignore_na=False).mean()\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "macd_indie = moving_average_convergence(pd.DataFrame(closep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wpr = williams_percent_r(closep)\n",
    "rsi = relative_strength_index(closep,  ROLLING / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "volatility1 = pd.DataFrame(closep).rolling(ROLLING).std().values#.tolist()\n",
    "volatility2 = pd.DataFrame(closep).rolling(ROLLING).var().values#.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "volatility = volatility1 / volatility2\n",
    "volatility = [v[0] for v in volatility]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rolling_skewness = pd.DataFrame(closep).rolling(ROLLING).skew().values \n",
    "rolling_kurtosis = pd.DataFrame(closep).rolling(ROLLING).kurt().values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 362 is out of bounds for axis 0 with size 362\n"
     ]
    }
   ],
   "source": [
    "X, Y = [], []\n",
    "for i in range(WINDOW, len(data_original)-1, STEP): \n",
    "    try:\n",
    "        o = openp[i:i+WINDOW]\n",
    "        h = highp[i:i+WINDOW]\n",
    "        l = lowp[i:i+WINDOW]\n",
    "        c = closep[i:i+WINDOW]\n",
    "        v = volumep[i:i+WINDOW]\n",
    "        volat = volatility[i:i+WINDOW]\n",
    "        rsk = rolling_skewness[i:i+WINDOW]\n",
    "        rku = rolling_kurtosis[i:i+WINDOW]\n",
    "        macd = macd_indie[i:i+WINDOW]\n",
    "        williams = wpr[i:i+WINDOW]\n",
    "        relative = rsi[i:i+WINDOW]\n",
    "        ichi = ichimoku[i:i+WINDOW]\n",
    "\n",
    "        #if closep[i+WINDOW+FORECAST] < 0.0001 or closep[i+WINDOW+FORECAST] > 10000:\n",
    "        #  continue\n",
    "\n",
    "\n",
    "        macd = remap(np.array(macd), np.array(macd).min(), np.array(macd).max(), -1, 1)\n",
    "        williams = remap(np.array(williams), np.array(williams).min(), np.array(williams).max(), -1, 1)\n",
    "        relative = remap(np.array(relative), np.array(relative).min(), np.array(relative).max(), -1, 1)\n",
    "        ichi = remap(np.array(ichi), np.array(ichi).min(), np.array(ichi).max(), -1, 1)\n",
    "        o = remap(np.array(o), np.array(o).min(), np.array(o).max(), -1, 1)\n",
    "        h = remap(np.array(h), np.array(h).min(), np.array(h).max(), -1, 1)\n",
    "        l = remap(np.array(l), np.array(l).min(), np.array(l).max(), -1, 1)\n",
    "        c = remap(np.array(c), np.array(c).min(), np.array(c).max(), -1, 1)\n",
    "        v = remap(np.array(v), np.array(v).min(), np.array(v).max(), -1, 1)\n",
    "        volat = remap(np.array(volat), np.array(volat).min(), np.array(volat).max(), -1, 1)\n",
    "        rsk = remap(np.array(rsk), np.array(rsk).min(), np.array(rsk).max(), -1, 1)\n",
    "        rku = remap(np.array(rku), np.array(rku).min(), np.array(rku).max(), -1, 1)\n",
    "\n",
    "        x_i = np.column_stack((o, h, l, c, v, volat, rsk, rku, macd, williams, relative, ichi))\n",
    "        # x_i = np.column_stack((o, h, l, c, v))\n",
    "        x_i = x_i.flatten()\n",
    "\n",
    "        # y_i = (closep[i+WINDOW+FORECAST] - closep[i+WINDOW]) / closep[i+WINDOW]\n",
    "        y_i = rolling_skewness[i+WINDOW+FORECAST]\n",
    "\n",
    "        # y_i = nolds.hurst_rs(closep[i:i+WINDOW+FORECAST])\n",
    "\n",
    "\n",
    "        if np.isnan(x_i).any() or np.isinf(x_i).any():\n",
    "          continue\n",
    "\n",
    "\n",
    "        if np.isnan(y_i).any() or np.isinf(y_i).any():\n",
    "            continue                \n",
    "\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        break\n",
    "\n",
    "    X.append(x_i)\n",
    "    Y.append(y_i)\n",
    "\n",
    "\n",
    "X, Y = np.array(X), np.array(Y)\n",
    "X_train, X_test, Y_train, Y_test = create_Xt_Yt(X, Y, TEST_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_input = Input(shape=(len(X[0]), ), name='main_input')\n",
    "x = GaussianNoise(0.05)(main_input)\n",
    "x = Lambda(lambda x: K.clip(x, min_value=-1, max_value=1))(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = GaussianNoise(0.05)(x)\n",
    "output = Dense(1, activation = \"linear\", name = \"out\")(x)\n",
    "\n",
    "final_model = Model(inputs=[main_input], outputs=[output])\n",
    "\n",
    "opt = Adam(lr=0.002)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=10, min_lr=0.000001, verbose=1)\n",
    "checkpointer = ModelCheckpoint(monitor='val_loss', filepath=\"xxx.hdf5\", verbose=1, save_best_only=True)\n",
    "\n",
    "final_model.compile(optimizer=opt, \n",
    "              loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.topology.InputLayer object at 0x124ea3c18> (None, 360)\n",
      "<keras.layers.noise.GaussianNoise object at 0x124ea32e8> (None, 360)\n",
      "<keras.layers.core.Lambda object at 0x124ea31d0> (None, 360)\n",
      "<keras.layers.core.Dense object at 0x124e7a828> (None, 64)\n",
      "<keras.layers.noise.GaussianNoise object at 0x124e844a8> (None, 64)\n",
      "<keras.layers.core.Dense object at 0x1250444a8> (None, 1)\n",
      "Train on 259 samples, validate on 29 samples\n",
      "Epoch 1/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.5482Epoch 00000: val_loss improved from inf to 0.37155, saving model to xxx.hdf5\n",
      "259/259 [==============================] - 0s - loss: 0.5535 - val_loss: 0.3715\n",
      "Epoch 2/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.3230Epoch 00001: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.3214 - val_loss: 0.4730\n",
      "Epoch 3/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.5126Epoch 00002: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.5076 - val_loss: 0.5673\n",
      "Epoch 4/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.6762Epoch 00003: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.6736 - val_loss: 0.3742\n",
      "Epoch 5/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.4468Epoch 00004: val_loss improved from 0.37155 to 0.31216, saving model to xxx.hdf5\n",
      "259/259 [==============================] - 0s - loss: 0.4474 - val_loss: 0.3122\n",
      "Epoch 6/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.2586Epoch 00005: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.2573 - val_loss: 0.4401\n",
      "Epoch 7/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.3312Epoch 00006: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.3297 - val_loss: 0.4954\n",
      "Epoch 8/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.3894Epoch 00007: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.3901 - val_loss: 0.3572\n",
      "Epoch 9/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.2837Epoch 00008: val_loss improved from 0.31216 to 0.24765, saving model to xxx.hdf5\n",
      "259/259 [==============================] - 0s - loss: 0.2848 - val_loss: 0.2476\n",
      "Epoch 10/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.2539Epoch 00009: val_loss improved from 0.24765 to 0.20918, saving model to xxx.hdf5\n",
      "259/259 [==============================] - 0s - loss: 0.2555 - val_loss: 0.2092\n",
      "Epoch 11/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.2332Epoch 00010: val_loss improved from 0.20918 to 0.19773, saving model to xxx.hdf5\n",
      "259/259 [==============================] - 0s - loss: 0.2350 - val_loss: 0.1977\n",
      "Epoch 12/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.2098Epoch 00011: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.2078 - val_loss: 0.2059\n",
      "Epoch 13/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.2475Epoch 00012: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.2456 - val_loss: 0.2284\n",
      "Epoch 14/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.2884Epoch 00013: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.2860 - val_loss: 0.2567\n",
      "Epoch 15/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.2813Epoch 00014: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.2784 - val_loss: 0.2831\n",
      "Epoch 16/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.2228Epoch 00015: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.2209 - val_loss: 0.3004\n",
      "Epoch 17/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.1886Epoch 00016: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.1882 - val_loss: 0.3063\n",
      "Epoch 18/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.1740Epoch 00017: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.1725 - val_loss: 0.2841\n",
      "Epoch 19/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.1683Epoch 00018: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.1691 - val_loss: 0.2540\n",
      "Epoch 20/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.1527Epoch 00019: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.1528 - val_loss: 0.2007\n",
      "Epoch 21/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.1377Epoch 00020: val_loss improved from 0.19773 to 0.16639, saving model to xxx.hdf5\n",
      "259/259 [==============================] - 0s - loss: 0.1370 - val_loss: 0.1664\n",
      "Epoch 22/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.1309Epoch 00021: val_loss improved from 0.16639 to 0.15418, saving model to xxx.hdf5\n",
      "259/259 [==============================] - 0s - loss: 0.1360 - val_loss: 0.1542\n",
      "Epoch 23/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.1274Epoch 00022: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.1323 - val_loss: 0.1662\n",
      "Epoch 24/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.1065Epoch 00023: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.1056 - val_loss: 0.1917\n",
      "Epoch 25/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.1312Epoch 00024: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.1320 - val_loss: 0.2065\n",
      "Epoch 26/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.1506Epoch 00025: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.1542 - val_loss: 0.2054\n",
      "Epoch 27/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.1468Epoch 00026: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.1452 - val_loss: 0.2190\n",
      "Epoch 28/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.1399Epoch 00027: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.1391 - val_loss: 0.2358\n",
      "Epoch 29/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.1444Epoch 00028: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.1457 - val_loss: 0.2331\n",
      "Epoch 30/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.1180Epoch 00029: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.1170 - val_loss: 0.2555\n",
      "Epoch 31/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00030: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0956 - val_loss: 0.2796\n",
      "Epoch 32/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.1022Epoch 00031: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.1015 - val_loss: 0.2711\n",
      "Epoch 33/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0831\n",
      "Epoch 00032: reducing learning rate to 0.0018000000854954123.\n",
      "Epoch 00032: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0834 - val_loss: 0.2678\n",
      "Epoch 34/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0789Epoch 00033: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0782 - val_loss: 0.2784\n",
      "Epoch 35/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0946Epoch 00034: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0953 - val_loss: 0.2870\n",
      "Epoch 36/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0935Epoch 00035: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0978 - val_loss: 0.2683\n",
      "Epoch 37/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0938Epoch 00036: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0932 - val_loss: 0.2426\n",
      "Epoch 38/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.1004Epoch 00037: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259/259 [==============================] - 0s - loss: 0.1013 - val_loss: 0.2323\n",
      "Epoch 39/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0961Epoch 00038: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0955 - val_loss: 0.2209\n",
      "Epoch 40/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0743Epoch 00039: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0743 - val_loss: 0.2100\n",
      "Epoch 41/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0748Epoch 00040: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0743 - val_loss: 0.2061\n",
      "Epoch 42/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0900Epoch 00041: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0897 - val_loss: 0.2074\n",
      "Epoch 43/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0911\n",
      "Epoch 00042: reducing learning rate to 0.0016200000769458712.\n",
      "Epoch 00042: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0905 - val_loss: 0.2099\n",
      "Epoch 44/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0842Epoch 00043: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0844 - val_loss: 0.2138\n",
      "Epoch 45/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0681Epoch 00044: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0683 - val_loss: 0.2207\n",
      "Epoch 46/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0693Epoch 00045: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0709 - val_loss: 0.2299\n",
      "Epoch 47/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0657Epoch 00046: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0654 - val_loss: 0.2419\n",
      "Epoch 48/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0579Epoch 00047: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0596 - val_loss: 0.2517\n",
      "Epoch 49/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0626Epoch 00048: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0623 - val_loss: 0.2521\n",
      "Epoch 50/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0694Epoch 00049: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0696 - val_loss: 0.2471\n",
      "Epoch 51/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0608Epoch 00050: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0601 - val_loss: 0.2419\n",
      "Epoch 52/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0447Epoch 00051: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0458 - val_loss: 0.2362\n",
      "Epoch 53/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0432\n",
      "Epoch 00052: reducing learning rate to 0.001458000100683421.\n",
      "Epoch 00052: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0427 - val_loss: 0.2325\n",
      "Epoch 54/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0433Epoch 00053: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0429 - val_loss: 0.2333\n",
      "Epoch 55/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0395Epoch 00054: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0399 - val_loss: 0.2347\n",
      "Epoch 56/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0373Epoch 00055: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0372 - val_loss: 0.2343\n",
      "Epoch 57/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0355Epoch 00056: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0353 - val_loss: 0.2337\n",
      "Epoch 58/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0358Epoch 00057: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0357 - val_loss: 0.2307\n",
      "Epoch 59/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0362Epoch 00058: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0362 - val_loss: 0.2248\n",
      "Epoch 60/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0418Epoch 00059: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0416 - val_loss: 0.2204\n",
      "Epoch 61/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0394Epoch 00060: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0391 - val_loss: 0.2207\n",
      "Epoch 62/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0342Epoch 00061: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0344 - val_loss: 0.2302\n",
      "Epoch 63/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0306\n",
      "Epoch 00062: reducing learning rate to 0.0013122001430019737.\n",
      "Epoch 00062: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0307 - val_loss: 0.2437\n",
      "Epoch 64/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0366Epoch 00063: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0371 - val_loss: 0.2455\n",
      "Epoch 65/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0321Epoch 00064: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0319 - val_loss: 0.2430\n",
      "Epoch 66/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0396Epoch 00065: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0395 - val_loss: 0.2434\n",
      "Epoch 67/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0509Epoch 00066: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0505 - val_loss: 0.2409\n",
      "Epoch 68/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0538Epoch 00067: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0538 - val_loss: 0.2368\n",
      "Epoch 69/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0430Epoch 00068: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0431 - val_loss: 0.2330\n",
      "Epoch 70/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0356Epoch 00069: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0353 - val_loss: 0.2294\n",
      "Epoch 71/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0317Epoch 00070: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0317 - val_loss: 0.2279\n",
      "Epoch 72/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0391Epoch 00071: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0394 - val_loss: 0.2243\n",
      "Epoch 73/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0382\n",
      "Epoch 00072: reducing learning rate to 0.0011809800867922605.\n",
      "Epoch 00072: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0393 - val_loss: 0.2187\n",
      "Epoch 74/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0299Epoch 00073: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0303 - val_loss: 0.2186\n",
      "Epoch 75/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0304Epoch 00074: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0301 - val_loss: 0.2209\n",
      "Epoch 76/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0286Epoch 00075: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0284 - val_loss: 0.2219\n",
      "Epoch 77/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0339Epoch 00076: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0340 - val_loss: 0.2240\n",
      "Epoch 78/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0316Epoch 00077: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0316 - val_loss: 0.2306\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/259 [============================>.] - ETA: 0s - loss: 0.0312Epoch 00078: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0319 - val_loss: 0.2371\n",
      "Epoch 80/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0365Epoch 00079: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0365 - val_loss: 0.2407\n",
      "Epoch 81/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0426Epoch 00080: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0422 - val_loss: 0.2427\n",
      "Epoch 82/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0460Epoch 00081: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0459 - val_loss: 0.2414\n",
      "Epoch 83/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0424\n",
      "Epoch 00082: reducing learning rate to 0.0010628821095451713.\n",
      "Epoch 00082: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0420 - val_loss: 0.2391\n",
      "Epoch 84/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0335Epoch 00083: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0332 - val_loss: 0.2375\n",
      "Epoch 85/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0271Epoch 00084: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0271 - val_loss: 0.2368\n",
      "Epoch 86/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0310Epoch 00085: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0308 - val_loss: 0.2351\n",
      "Epoch 87/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0332Epoch 00086: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0336 - val_loss: 0.2389\n",
      "Epoch 88/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0350Epoch 00087: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0349 - val_loss: 0.2520\n",
      "Epoch 89/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0333Epoch 00088: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0334 - val_loss: 0.2675\n",
      "Epoch 90/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0325Epoch 00089: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0325 - val_loss: 0.2766\n",
      "Epoch 91/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0365Epoch 00090: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0362 - val_loss: 0.2785\n",
      "Epoch 92/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0394Epoch 00091: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0394 - val_loss: 0.2686\n",
      "Epoch 93/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0355\n",
      "Epoch 00092: reducing learning rate to 0.0009565939195454121.\n",
      "Epoch 00092: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0353 - val_loss: 0.2482\n",
      "Epoch 94/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0291Epoch 00093: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0295 - val_loss: 0.2336\n",
      "Epoch 95/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0252Epoch 00094: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0254 - val_loss: 0.2256\n",
      "Epoch 96/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0236Epoch 00095: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0237 - val_loss: 0.2235\n",
      "Epoch 97/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0338Epoch 00096: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0336 - val_loss: 0.2239\n",
      "Epoch 98/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0449Epoch 00097: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0447 - val_loss: 0.2203\n",
      "Epoch 99/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0455Epoch 00098: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0455 - val_loss: 0.2153\n",
      "Epoch 100/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0408Epoch 00099: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0408 - val_loss: 0.2147\n",
      "Epoch 101/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0280Epoch 00100: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0282 - val_loss: 0.2214\n",
      "Epoch 102/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0254Epoch 00101: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0252 - val_loss: 0.2344\n",
      "Epoch 103/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0298\n",
      "Epoch 00102: reducing learning rate to 0.000860934506636113.\n",
      "Epoch 00102: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0308 - val_loss: 0.2469\n",
      "Epoch 104/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0357Epoch 00103: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0355 - val_loss: 0.2542\n",
      "Epoch 105/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0313Epoch 00104: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0312 - val_loss: 0.2590\n",
      "Epoch 106/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0312Epoch 00105: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0312 - val_loss: 0.2575\n",
      "Epoch 107/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0307Epoch 00106: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0311 - val_loss: 0.2535\n",
      "Epoch 108/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0319Epoch 00107: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0316 - val_loss: 0.2509\n",
      "Epoch 109/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0314Epoch 00108: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0315 - val_loss: 0.2517\n",
      "Epoch 110/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0316Epoch 00109: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0313 - val_loss: 0.2546\n",
      "Epoch 111/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0266Epoch 00110: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0266 - val_loss: 0.2558\n",
      "Epoch 112/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0211Epoch 00111: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0215 - val_loss: 0.2549\n",
      "Epoch 113/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0213\n",
      "Epoch 00112: reducing learning rate to 0.0007748410454951227.\n",
      "Epoch 00112: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0214 - val_loss: 0.2537\n",
      "Epoch 114/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0228Epoch 00113: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0227 - val_loss: 0.2530\n",
      "Epoch 115/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0258Epoch 00114: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0258 - val_loss: 0.2514\n",
      "Epoch 116/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0239Epoch 00115: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0238 - val_loss: 0.2471\n",
      "Epoch 117/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0221Epoch 00116: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0219 - val_loss: 0.2419\n",
      "Epoch 118/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0200Epoch 00117: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0199 - val_loss: 0.2388\n",
      "Epoch 119/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0219Epoch 00118: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259/259 [==============================] - 0s - loss: 0.0220 - val_loss: 0.2385\n",
      "Epoch 120/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0201Epoch 00119: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0201 - val_loss: 0.2413\n",
      "Epoch 121/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0215Epoch 00120: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0214 - val_loss: 0.2461\n",
      "Epoch 122/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0202Epoch 00121: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0204 - val_loss: 0.2507\n",
      "Epoch 123/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0203\n",
      "Epoch 00122: reducing learning rate to 0.0006973569514229894.\n",
      "Epoch 00122: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0201 - val_loss: 0.2538\n",
      "Epoch 124/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0180Epoch 00123: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0179 - val_loss: 0.2538\n",
      "Epoch 125/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0202Epoch 00124: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0201 - val_loss: 0.2526\n",
      "Epoch 126/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0192Epoch 00125: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0194 - val_loss: 0.2501\n",
      "Epoch 127/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0183Epoch 00126: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0181 - val_loss: 0.2465\n",
      "Epoch 128/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0198Epoch 00127: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0199 - val_loss: 0.2421\n",
      "Epoch 129/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0195Epoch 00128: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0193 - val_loss: 0.2375\n",
      "Epoch 130/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0176Epoch 00129: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0176 - val_loss: 0.2335\n",
      "Epoch 131/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0169Epoch 00130: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0168 - val_loss: 0.2301\n",
      "Epoch 132/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0144Epoch 00131: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0145 - val_loss: 0.2280\n",
      "Epoch 133/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0159\n",
      "Epoch 00132: reducing learning rate to 0.0006276212458033115.\n",
      "Epoch 00132: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0158 - val_loss: 0.2264\n",
      "Epoch 134/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0150Epoch 00133: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0148 - val_loss: 0.2257\n",
      "Epoch 135/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0165Epoch 00134: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0164 - val_loss: 0.2259\n",
      "Epoch 136/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0172Epoch 00135: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0171 - val_loss: 0.2267\n",
      "Epoch 137/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0177Epoch 00136: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0177 - val_loss: 0.2273\n",
      "Epoch 138/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0169Epoch 00137: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0168 - val_loss: 0.2264\n",
      "Epoch 139/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0153Epoch 00138: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0151 - val_loss: 0.2252\n",
      "Epoch 140/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0144Epoch 00139: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0143 - val_loss: 0.2250\n",
      "Epoch 141/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0135Epoch 00140: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0134 - val_loss: 0.2252\n",
      "Epoch 142/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0163Epoch 00141: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0163 - val_loss: 0.2254\n",
      "Epoch 143/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0152\n",
      "Epoch 00142: reducing learning rate to 0.0005648591264616698.\n",
      "Epoch 00142: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0151 - val_loss: 0.2258\n",
      "Epoch 144/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0147Epoch 00143: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0148 - val_loss: 0.2253\n",
      "Epoch 145/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0165Epoch 00144: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0166 - val_loss: 0.2229\n",
      "Epoch 146/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0155Epoch 00145: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0155 - val_loss: 0.2210\n",
      "Epoch 147/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0135Epoch 00146: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0139 - val_loss: 0.2198\n",
      "Epoch 148/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0132Epoch 00147: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0131 - val_loss: 0.2184\n",
      "Epoch 149/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0140Epoch 00148: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0139 - val_loss: 0.2173\n",
      "Epoch 150/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0136Epoch 00149: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0136 - val_loss: 0.2171\n",
      "Epoch 151/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0130Epoch 00150: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0129 - val_loss: 0.2181\n",
      "Epoch 152/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0123Epoch 00151: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0127 - val_loss: 0.2186\n",
      "Epoch 153/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0161\n",
      "Epoch 00152: reducing learning rate to 0.0005083732190541923.\n",
      "Epoch 00152: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0161 - val_loss: 0.2177\n",
      "Epoch 154/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0146Epoch 00153: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0147 - val_loss: 0.2154\n",
      "Epoch 155/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0148Epoch 00154: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0150 - val_loss: 0.2108\n",
      "Epoch 156/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0132Epoch 00155: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0131 - val_loss: 0.2064\n",
      "Epoch 157/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0133Epoch 00156: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0132 - val_loss: 0.2037\n",
      "Epoch 158/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0159Epoch 00157: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0159 - val_loss: 0.2011\n",
      "Epoch 159/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0170Epoch 00158: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259/259 [==============================] - 0s - loss: 0.0169 - val_loss: 0.1992\n",
      "Epoch 160/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0166Epoch 00159: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0169 - val_loss: 0.1999\n",
      "Epoch 161/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0172Epoch 00160: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0172 - val_loss: 0.2035\n",
      "Epoch 162/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0136Epoch 00161: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0135 - val_loss: 0.2072\n",
      "Epoch 163/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0150\n",
      "Epoch 00162: reducing learning rate to 0.00045753587619401515.\n",
      "Epoch 00162: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0149 - val_loss: 0.2093\n",
      "Epoch 164/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0120Epoch 00163: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0120 - val_loss: 0.2101\n",
      "Epoch 165/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0124Epoch 00164: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0124 - val_loss: 0.2108\n",
      "Epoch 166/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0123Epoch 00165: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0123 - val_loss: 0.2111\n",
      "Epoch 167/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0128Epoch 00166: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0128 - val_loss: 0.2110\n",
      "Epoch 168/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0112Epoch 00167: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0111 - val_loss: 0.2113\n",
      "Epoch 169/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0127Epoch 00168: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0132 - val_loss: 0.2127\n",
      "Epoch 170/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0118Epoch 00169: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0121 - val_loss: 0.2149\n",
      "Epoch 171/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0119Epoch 00170: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0119 - val_loss: 0.2169\n",
      "Epoch 172/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0140Epoch 00171: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0140 - val_loss: 0.2185\n",
      "Epoch 173/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0134\n",
      "Epoch 00172: reducing learning rate to 0.00041178228857461366.\n",
      "Epoch 00172: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0134 - val_loss: 0.2204\n",
      "Epoch 174/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0109Epoch 00173: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0108 - val_loss: 0.2222\n",
      "Epoch 175/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0116Epoch 00174: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0116 - val_loss: 0.2244\n",
      "Epoch 176/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0114Epoch 00175: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0113 - val_loss: 0.2273\n",
      "Epoch 177/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0119Epoch 00176: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0119 - val_loss: 0.2292\n",
      "Epoch 178/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0120Epoch 00177: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0120 - val_loss: 0.2308\n",
      "Epoch 179/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0125Epoch 00178: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0124 - val_loss: 0.2329\n",
      "Epoch 180/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0136Epoch 00179: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0138 - val_loss: 0.2339\n",
      "Epoch 181/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0133Epoch 00180: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0132 - val_loss: 0.2337\n",
      "Epoch 182/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0145Epoch 00181: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0143 - val_loss: 0.2331\n",
      "Epoch 183/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0120\n",
      "Epoch 00182: reducing learning rate to 0.0003706040675751865.\n",
      "Epoch 00182: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0121 - val_loss: 0.2318\n",
      "Epoch 184/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0111Epoch 00183: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0110 - val_loss: 0.2308\n",
      "Epoch 185/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0109Epoch 00184: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0114 - val_loss: 0.2306\n",
      "Epoch 186/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0103Epoch 00185: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0102 - val_loss: 0.2305\n",
      "Epoch 187/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0119Epoch 00186: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0118 - val_loss: 0.2305\n",
      "Epoch 188/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0115Epoch 00187: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0114 - val_loss: 0.2306\n",
      "Epoch 189/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0098Epoch 00188: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0098 - val_loss: 0.2315\n",
      "Epoch 190/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0103Epoch 00189: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0103 - val_loss: 0.2331\n",
      "Epoch 191/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0089Epoch 00190: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0089 - val_loss: 0.2342\n",
      "Epoch 192/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0101Epoch 00191: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0103 - val_loss: 0.2350\n",
      "Epoch 193/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 00192: reducing learning rate to 0.0003335436660563573.\n",
      "Epoch 00192: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0110 - val_loss: 0.2356\n",
      "Epoch 194/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0091Epoch 00193: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0091 - val_loss: 0.2361\n",
      "Epoch 195/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0111Epoch 00194: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0111 - val_loss: 0.2361\n",
      "Epoch 196/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0103Epoch 00195: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0102 - val_loss: 0.2358\n",
      "Epoch 197/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0121Epoch 00196: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0121 - val_loss: 0.2352\n",
      "Epoch 198/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0105Epoch 00197: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0104 - val_loss: 0.2341\n",
      "Epoch 199/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0103Epoch 00198: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259/259 [==============================] - 0s - loss: 0.0102 - val_loss: 0.2330\n",
      "Epoch 200/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0112Epoch 00199: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0112 - val_loss: 0.2323\n",
      "Epoch 201/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0109Epoch 00200: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0108 - val_loss: 0.2324\n",
      "Epoch 202/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0102Epoch 00201: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0101 - val_loss: 0.2332\n",
      "Epoch 203/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0105\n",
      "Epoch 00202: reducing learning rate to 0.0003001892968313769.\n",
      "Epoch 00202: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0105 - val_loss: 0.2340\n",
      "Epoch 204/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0105Epoch 00203: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0108 - val_loss: 0.2339\n",
      "Epoch 205/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0101Epoch 00204: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0101 - val_loss: 0.2337\n",
      "Epoch 206/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0100Epoch 00205: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0100 - val_loss: 0.2331\n",
      "Epoch 207/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0095Epoch 00206: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0095 - val_loss: 0.2315\n",
      "Epoch 208/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0092Epoch 00207: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0093 - val_loss: 0.2291\n",
      "Epoch 209/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0095Epoch 00208: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0094 - val_loss: 0.2265\n",
      "Epoch 210/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0102Epoch 00209: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0101 - val_loss: 0.2242\n",
      "Epoch 211/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0101Epoch 00210: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0102 - val_loss: 0.2232\n",
      "Epoch 212/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0118Epoch 00211: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0117 - val_loss: 0.2237\n",
      "Epoch 213/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 00212: reducing learning rate to 0.0002701703750062734.\n",
      "Epoch 00212: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0098 - val_loss: 0.2247\n",
      "Epoch 214/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0092Epoch 00213: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0091 - val_loss: 0.2260\n",
      "Epoch 215/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0092Epoch 00214: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0092 - val_loss: 0.2270\n",
      "Epoch 216/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0102Epoch 00215: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0101 - val_loss: 0.2278\n",
      "Epoch 217/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0097Epoch 00216: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0098 - val_loss: 0.2285\n",
      "Epoch 218/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0098Epoch 00217: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0099 - val_loss: 0.2290\n",
      "Epoch 219/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0088Epoch 00218: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0089 - val_loss: 0.2293\n",
      "Epoch 220/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0092Epoch 00219: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0092 - val_loss: 0.2294\n",
      "Epoch 221/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0113Epoch 00220: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0114 - val_loss: 0.2298\n",
      "Epoch 222/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0095Epoch 00221: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0094 - val_loss: 0.2303\n",
      "Epoch 223/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 00222: reducing learning rate to 0.0002431533270282671.\n",
      "Epoch 00222: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0096 - val_loss: 0.2310\n",
      "Epoch 224/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0102Epoch 00223: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0101 - val_loss: 0.2314\n",
      "Epoch 225/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0098Epoch 00224: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0098 - val_loss: 0.2315\n",
      "Epoch 226/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0092Epoch 00225: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0092 - val_loss: 0.2315\n",
      "Epoch 227/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0102Epoch 00226: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0101 - val_loss: 0.2316\n",
      "Epoch 228/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0091Epoch 00227: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0091 - val_loss: 0.2315\n",
      "Epoch 229/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0086Epoch 00228: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0086 - val_loss: 0.2307\n",
      "Epoch 230/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0088Epoch 00229: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0089 - val_loss: 0.2296\n",
      "Epoch 231/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0089Epoch 00230: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2285\n",
      "Epoch 232/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0104Epoch 00231: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0103 - val_loss: 0.2279\n",
      "Epoch 233/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0082\n",
      "Epoch 00232: reducing learning rate to 0.0002188379890867509.\n",
      "Epoch 00232: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0081 - val_loss: 0.2278\n",
      "Epoch 234/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0089Epoch 00233: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0089 - val_loss: 0.2280\n",
      "Epoch 235/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0087Epoch 00234: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0086 - val_loss: 0.2284\n",
      "Epoch 236/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0086Epoch 00235: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0085 - val_loss: 0.2291\n",
      "Epoch 237/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0079Epoch 00236: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0079 - val_loss: 0.2303\n",
      "Epoch 238/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0097Epoch 00237: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0096 - val_loss: 0.2320\n",
      "Epoch 239/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0106Epoch 00238: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259/259 [==============================] - 0s - loss: 0.0105 - val_loss: 0.2336\n",
      "Epoch 240/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0087Epoch 00239: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0087 - val_loss: 0.2346\n",
      "Epoch 241/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0098Epoch 00240: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0099 - val_loss: 0.2352\n",
      "Epoch 242/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0094Epoch 00241: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0094 - val_loss: 0.2359\n",
      "Epoch 243/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0080\n",
      "Epoch 00242: reducing learning rate to 0.00019695418886840345.\n",
      "Epoch 00242: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0079 - val_loss: 0.2366\n",
      "Epoch 244/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0087Epoch 00243: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2373\n",
      "Epoch 245/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0092Epoch 00244: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0092 - val_loss: 0.2383\n",
      "Epoch 246/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0088Epoch 00245: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0089 - val_loss: 0.2398\n",
      "Epoch 247/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0089Epoch 00246: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2414\n",
      "Epoch 248/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0086Epoch 00247: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2426\n",
      "Epoch 249/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0096Epoch 00248: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0097 - val_loss: 0.2429\n",
      "Epoch 250/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0088Epoch 00249: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2424\n",
      "Epoch 251/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0097Epoch 00250: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0098 - val_loss: 0.2414\n",
      "Epoch 252/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0097Epoch 00251: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0096 - val_loss: 0.2403\n",
      "Epoch 253/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 00252: reducing learning rate to 0.00017725877260090783.\n",
      "Epoch 00252: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2388\n",
      "Epoch 254/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0090Epoch 00253: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0090 - val_loss: 0.2371\n",
      "Epoch 255/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0094Epoch 00254: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0093 - val_loss: 0.2357\n",
      "Epoch 256/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0081Epoch 00255: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0082 - val_loss: 0.2342\n",
      "Epoch 257/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0093Epoch 00256: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0094 - val_loss: 0.2326\n",
      "Epoch 258/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0095Epoch 00257: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0095 - val_loss: 0.2314\n",
      "Epoch 259/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0089Epoch 00258: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2304\n",
      "Epoch 260/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0097Epoch 00259: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0097 - val_loss: 0.2296\n",
      "Epoch 261/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0090Epoch 00260: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0092 - val_loss: 0.2292\n",
      "Epoch 262/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0092Epoch 00261: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0093 - val_loss: 0.2290\n",
      "Epoch 263/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0083\n",
      "Epoch 00262: reducing learning rate to 0.00015953289403114468.\n",
      "Epoch 00262: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0083 - val_loss: 0.2288\n",
      "Epoch 264/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0099Epoch 00263: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0099 - val_loss: 0.2284\n",
      "Epoch 265/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0083Epoch 00264: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0083 - val_loss: 0.2280\n",
      "Epoch 266/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0078Epoch 00265: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2277\n",
      "Epoch 267/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0093Epoch 00266: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0092 - val_loss: 0.2276\n",
      "Epoch 268/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0095Epoch 00267: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0095 - val_loss: 0.2275\n",
      "Epoch 269/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0079Epoch 00268: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0079 - val_loss: 0.2272\n",
      "Epoch 270/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0090Epoch 00269: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0091 - val_loss: 0.2267\n",
      "Epoch 271/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0087Epoch 00270: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0086 - val_loss: 0.2262\n",
      "Epoch 272/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0085Epoch 00271: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0085 - val_loss: 0.2257\n",
      "Epoch 273/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 00272: reducing learning rate to 0.00014357960462803022.\n",
      "Epoch 00272: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0072 - val_loss: 0.2250\n",
      "Epoch 274/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0082Epoch 00273: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0081 - val_loss: 0.2242\n",
      "Epoch 275/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0083Epoch 00274: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0083 - val_loss: 0.2237\n",
      "Epoch 276/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0082Epoch 00275: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0081 - val_loss: 0.2236\n",
      "Epoch 277/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0076Epoch 00276: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2237\n",
      "Epoch 278/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0090Epoch 00277: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0090 - val_loss: 0.2241\n",
      "Epoch 279/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0085Epoch 00278: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259/259 [==============================] - 0s - loss: 0.0086 - val_loss: 0.2244\n",
      "Epoch 280/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0088Epoch 00279: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2247\n",
      "Epoch 281/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0084Epoch 00280: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0084 - val_loss: 0.2252\n",
      "Epoch 282/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0092Epoch 00281: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0092 - val_loss: 0.2251\n",
      "Epoch 283/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 00282: reducing learning rate to 0.00012922164023621008.\n",
      "Epoch 00282: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0096 - val_loss: 0.2252\n",
      "Epoch 284/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0079Epoch 00283: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2257\n",
      "Epoch 285/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0070Epoch 00284: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0071 - val_loss: 0.2260\n",
      "Epoch 286/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00285: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2267\n",
      "Epoch 287/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0098Epoch 00286: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0097 - val_loss: 0.2279\n",
      "Epoch 288/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0081Epoch 00287: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0081 - val_loss: 0.2290\n",
      "Epoch 289/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0082Epoch 00288: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0081 - val_loss: 0.2298\n",
      "Epoch 290/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00289: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2297\n",
      "Epoch 291/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0079Epoch 00290: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2291\n",
      "Epoch 292/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0084Epoch 00291: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0084 - val_loss: 0.2287\n",
      "Epoch 293/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0089\n",
      "Epoch 00292: reducing learning rate to 0.00011629948276095093.\n",
      "Epoch 00292: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2282\n",
      "Epoch 294/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0088Epoch 00293: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0087 - val_loss: 0.2278\n",
      "Epoch 295/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0081Epoch 00294: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0080 - val_loss: 0.2274\n",
      "Epoch 296/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0080Epoch 00295: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0080 - val_loss: 0.2272\n",
      "Epoch 297/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0078Epoch 00296: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2271\n",
      "Epoch 298/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0078Epoch 00297: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0081 - val_loss: 0.2270\n",
      "Epoch 299/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0086Epoch 00298: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0087 - val_loss: 0.2271\n",
      "Epoch 300/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0080Epoch 00299: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0081 - val_loss: 0.2270\n",
      "Epoch 301/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0098Epoch 00300: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0098 - val_loss: 0.2263\n",
      "Epoch 302/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0080Epoch 00301: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0079 - val_loss: 0.2258\n",
      "Epoch 303/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0081\n",
      "Epoch 00302: reducing learning rate to 0.00010466953317518346.\n",
      "Epoch 00302: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0080 - val_loss: 0.2251\n",
      "Epoch 304/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0081Epoch 00303: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0083 - val_loss: 0.2242\n",
      "Epoch 305/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0086Epoch 00304: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0085 - val_loss: 0.2232\n",
      "Epoch 306/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0093Epoch 00305: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0093 - val_loss: 0.2230\n",
      "Epoch 307/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0088Epoch 00306: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2230\n",
      "Epoch 308/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0086Epoch 00307: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0085 - val_loss: 0.2228\n",
      "Epoch 309/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0098Epoch 00308: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0097 - val_loss: 0.2226\n",
      "Epoch 310/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0087Epoch 00309: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2228\n",
      "Epoch 311/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0086Epoch 00310: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0086 - val_loss: 0.2233\n",
      "Epoch 312/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0078Epoch 00311: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0079 - val_loss: 0.2239\n",
      "Epoch 313/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0077\n",
      "Epoch 00312: reducing learning rate to 9.420257920282893e-05.\n",
      "Epoch 00312: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2246\n",
      "Epoch 314/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00313: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0076 - val_loss: 0.2252\n",
      "Epoch 315/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0076Epoch 00314: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0076 - val_loss: 0.2258\n",
      "Epoch 316/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00315: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0076 - val_loss: 0.2262\n",
      "Epoch 317/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0066Epoch 00316: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0065 - val_loss: 0.2265\n",
      "Epoch 318/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0080Epoch 00317: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0081 - val_loss: 0.2263\n",
      "Epoch 319/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0083Epoch 00318: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259/259 [==============================] - 0s - loss: 0.0083 - val_loss: 0.2258\n",
      "Epoch 320/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0091Epoch 00319: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0091 - val_loss: 0.2252\n",
      "Epoch 321/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0096Epoch 00320: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0096 - val_loss: 0.2243\n",
      "Epoch 322/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0098Epoch 00321: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0098 - val_loss: 0.2231\n",
      "Epoch 323/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0073\n",
      "Epoch 00322: reducing learning rate to 8.478232193738222e-05.\n",
      "Epoch 00322: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0072 - val_loss: 0.2222\n",
      "Epoch 324/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0086Epoch 00323: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0085 - val_loss: 0.2217\n",
      "Epoch 325/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0082Epoch 00324: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0082 - val_loss: 0.2214\n",
      "Epoch 326/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0090Epoch 00325: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0089 - val_loss: 0.2210\n",
      "Epoch 327/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0088Epoch 00326: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2209\n",
      "Epoch 328/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0079Epoch 00327: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2210\n",
      "Epoch 329/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0088Epoch 00328: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0087 - val_loss: 0.2213\n",
      "Epoch 330/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0078Epoch 00329: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2216\n",
      "Epoch 331/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00330: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2219\n",
      "Epoch 332/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00331: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2222\n",
      "Epoch 333/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 00332: reducing learning rate to 7.630409236298874e-05.\n",
      "Epoch 00332: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0087 - val_loss: 0.2228\n",
      "Epoch 334/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0065Epoch 00333: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0065 - val_loss: 0.2235\n",
      "Epoch 335/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00334: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0076 - val_loss: 0.2242\n",
      "Epoch 336/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0068Epoch 00335: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0070 - val_loss: 0.2253\n",
      "Epoch 337/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0069Epoch 00336: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0068 - val_loss: 0.2262\n",
      "Epoch 338/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00337: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0076 - val_loss: 0.2268\n",
      "Epoch 339/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0079Epoch 00338: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0080 - val_loss: 0.2272\n",
      "Epoch 340/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0082Epoch 00339: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0082 - val_loss: 0.2277\n",
      "Epoch 341/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0086Epoch 00340: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0086 - val_loss: 0.2284\n",
      "Epoch 342/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00341: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2292\n",
      "Epoch 343/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075\n",
      "Epoch 00342: reducing learning rate to 6.867368574603461e-05.\n",
      "Epoch 00342: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2300\n",
      "Epoch 344/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0079Epoch 00343: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2304\n",
      "Epoch 345/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0073Epoch 00344: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0072 - val_loss: 0.2305\n",
      "Epoch 346/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0088Epoch 00345: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0090 - val_loss: 0.2304\n",
      "Epoch 347/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0081Epoch 00346: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0083 - val_loss: 0.2302\n",
      "Epoch 348/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0067Epoch 00347: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0067 - val_loss: 0.2304\n",
      "Epoch 349/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0071Epoch 00348: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0071 - val_loss: 0.2309\n",
      "Epoch 350/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0080Epoch 00349: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0080 - val_loss: 0.2312\n",
      "Epoch 351/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00350: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0076 - val_loss: 0.2314\n",
      "Epoch 352/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0078Epoch 00351: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2316\n",
      "Epoch 353/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0084\n",
      "Epoch 00352: reducing learning rate to 6.180632044561207e-05.\n",
      "Epoch 00352: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0084 - val_loss: 0.2317\n",
      "Epoch 354/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0088Epoch 00353: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0087 - val_loss: 0.2317\n",
      "Epoch 355/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00354: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2316\n",
      "Epoch 356/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0076Epoch 00355: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2315\n",
      "Epoch 357/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0085Epoch 00356: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0085 - val_loss: 0.2316\n",
      "Epoch 358/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0074Epoch 00357: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0074 - val_loss: 0.2318\n",
      "Epoch 359/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0085Epoch 00358: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259/259 [==============================] - 0s - loss: 0.0086 - val_loss: 0.2321\n",
      "Epoch 360/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0080Epoch 00359: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0079 - val_loss: 0.2325\n",
      "Epoch 361/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0078Epoch 00360: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2329\n",
      "Epoch 362/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0081Epoch 00361: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0081 - val_loss: 0.2331\n",
      "Epoch 363/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0077\n",
      "Epoch 00362: reducing learning rate to 5.562568840105087e-05.\n",
      "Epoch 00362: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2331\n",
      "Epoch 364/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0083Epoch 00363: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0083 - val_loss: 0.2329\n",
      "Epoch 365/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0071Epoch 00364: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0072 - val_loss: 0.2328\n",
      "Epoch 366/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0079Epoch 00365: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0079 - val_loss: 0.2324\n",
      "Epoch 367/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0084Epoch 00366: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0083 - val_loss: 0.2320\n",
      "Epoch 368/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0081Epoch 00367: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0080 - val_loss: 0.2315\n",
      "Epoch 369/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0071Epoch 00368: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0071 - val_loss: 0.2313\n",
      "Epoch 370/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0072Epoch 00369: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0071 - val_loss: 0.2312\n",
      "Epoch 371/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0073Epoch 00370: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0073 - val_loss: 0.2309\n",
      "Epoch 372/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0076Epoch 00371: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2304\n",
      "Epoch 373/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0082\n",
      "Epoch 00372: reducing learning rate to 5.006312021578197e-05.\n",
      "Epoch 00372: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0083 - val_loss: 0.2302\n",
      "Epoch 374/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0063Epoch 00373: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0062 - val_loss: 0.2302\n",
      "Epoch 375/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0071Epoch 00374: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0072 - val_loss: 0.2302\n",
      "Epoch 376/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0078Epoch 00375: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2301\n",
      "Epoch 377/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0073Epoch 00376: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0072 - val_loss: 0.2300\n",
      "Epoch 378/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0083Epoch 00377: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0084 - val_loss: 0.2298\n",
      "Epoch 379/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0068Epoch 00378: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0067 - val_loss: 0.2294\n",
      "Epoch 380/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0080Epoch 00379: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0081 - val_loss: 0.2291\n",
      "Epoch 381/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0079Epoch 00380: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0079 - val_loss: 0.2289\n",
      "Epoch 382/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0074Epoch 00381: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2288\n",
      "Epoch 383/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 00382: reducing learning rate to 4.505680917645805e-05.\n",
      "Epoch 00382: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0071 - val_loss: 0.2288\n",
      "Epoch 384/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0062Epoch 00383: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0064 - val_loss: 0.2291\n",
      "Epoch 385/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0064Epoch 00384: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0067 - val_loss: 0.2297\n",
      "Epoch 386/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0074Epoch 00385: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0073 - val_loss: 0.2301\n",
      "Epoch 387/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0076Epoch 00386: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0079 - val_loss: 0.2303\n",
      "Epoch 388/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0082Epoch 00387: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0082 - val_loss: 0.2303\n",
      "Epoch 389/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0073Epoch 00388: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0073 - val_loss: 0.2303\n",
      "Epoch 390/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0070Epoch 00389: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0069 - val_loss: 0.2303\n",
      "Epoch 391/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0071Epoch 00390: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0072 - val_loss: 0.2303\n",
      "Epoch 392/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0079Epoch 00391: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0079 - val_loss: 0.2301\n",
      "Epoch 393/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0074\n",
      "Epoch 00392: reducing learning rate to 4.055112694913987e-05.\n",
      "Epoch 00392: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0074 - val_loss: 0.2298\n",
      "Epoch 394/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00393: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2295\n",
      "Epoch 395/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00394: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0076 - val_loss: 0.2292\n",
      "Epoch 396/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0070Epoch 00395: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0070 - val_loss: 0.2289\n",
      "Epoch 397/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00396: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0076 - val_loss: 0.2289\n",
      "Epoch 398/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0073Epoch 00397: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0073 - val_loss: 0.2290\n",
      "Epoch 399/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0073Epoch 00398: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259/259 [==============================] - 0s - loss: 0.0073 - val_loss: 0.2290\n",
      "Epoch 400/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0066Epoch 00399: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0066 - val_loss: 0.2291\n",
      "Epoch 401/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0083Epoch 00400: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0083 - val_loss: 0.2291\n",
      "Epoch 402/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0080Epoch 00401: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0080 - val_loss: 0.2290\n",
      "Epoch 403/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0074\n",
      "Epoch 00402: reducing learning rate to 3.6496014581643975e-05.\n",
      "Epoch 00402: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2290\n",
      "Epoch 404/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00403: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2290\n",
      "Epoch 405/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0061Epoch 00404: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0060 - val_loss: 0.2290\n",
      "Epoch 406/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0069Epoch 00405: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0068 - val_loss: 0.2290\n",
      "Epoch 407/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00406: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2289\n",
      "Epoch 408/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0071Epoch 00407: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0073 - val_loss: 0.2289\n",
      "Epoch 409/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0071Epoch 00408: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0070 - val_loss: 0.2290\n",
      "Epoch 410/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0065Epoch 00409: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0064 - val_loss: 0.2291\n",
      "Epoch 411/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0078Epoch 00410: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2291\n",
      "Epoch 412/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0076Epoch 00411: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0076 - val_loss: 0.2292\n",
      "Epoch 413/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0076\n",
      "Epoch 00412: reducing learning rate to 3.284641279606149e-05.\n",
      "Epoch 00412: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0076 - val_loss: 0.2293\n",
      "Epoch 414/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00413: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2292\n",
      "Epoch 415/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00414: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2289\n",
      "Epoch 416/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0070Epoch 00415: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0070 - val_loss: 0.2286\n",
      "Epoch 417/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0083Epoch 00416: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0082 - val_loss: 0.2282\n",
      "Epoch 418/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0057Epoch 00417: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0058 - val_loss: 0.2279\n",
      "Epoch 419/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0067Epoch 00418: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0066 - val_loss: 0.2278\n",
      "Epoch 420/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0072Epoch 00419: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0073 - val_loss: 0.2278\n",
      "Epoch 421/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0070Epoch 00420: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0070 - val_loss: 0.2278\n",
      "Epoch 422/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00421: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0076 - val_loss: 0.2279\n",
      "Epoch 423/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0074\n",
      "Epoch 00422: reducing learning rate to 2.9561770861619153e-05.\n",
      "Epoch 00422: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0074 - val_loss: 0.2278\n",
      "Epoch 424/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0074Epoch 00423: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2278\n",
      "Epoch 425/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00424: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0076 - val_loss: 0.2278\n",
      "Epoch 426/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0070Epoch 00425: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0070 - val_loss: 0.2278\n",
      "Epoch 427/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0072Epoch 00426: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0072 - val_loss: 0.2279\n",
      "Epoch 428/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0082Epoch 00427: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0081 - val_loss: 0.2281\n",
      "Epoch 429/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0080Epoch 00428: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0080 - val_loss: 0.2282\n",
      "Epoch 430/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0066Epoch 00429: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0065 - val_loss: 0.2282\n",
      "Epoch 431/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00430: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2282\n",
      "Epoch 432/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0064Epoch 00431: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0065 - val_loss: 0.2283\n",
      "Epoch 433/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 00432: reducing learning rate to 2.660559312062105e-05.\n",
      "Epoch 00432: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0070 - val_loss: 0.2283\n",
      "Epoch 434/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00433: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0074 - val_loss: 0.2282\n",
      "Epoch 435/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0059Epoch 00434: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0059 - val_loss: 0.2282\n",
      "Epoch 436/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0078Epoch 00435: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2283\n",
      "Epoch 437/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0076Epoch 00436: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0076 - val_loss: 0.2284\n",
      "Epoch 438/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0074Epoch 00437: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0074 - val_loss: 0.2284\n",
      "Epoch 439/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0072Epoch 00438: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259/259 [==============================] - 0s - loss: 0.0071 - val_loss: 0.2284\n",
      "Epoch 440/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0074Epoch 00439: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0074 - val_loss: 0.2284\n",
      "Epoch 441/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0066Epoch 00440: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0066 - val_loss: 0.2284\n",
      "Epoch 442/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0072Epoch 00441: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0071 - val_loss: 0.2284\n",
      "Epoch 443/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0062\n",
      "Epoch 00442: reducing learning rate to 2.3945034627104178e-05.\n",
      "Epoch 00442: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0062 - val_loss: 0.2284\n",
      "Epoch 444/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0073Epoch 00443: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0073 - val_loss: 0.2283\n",
      "Epoch 445/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0069Epoch 00444: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0069 - val_loss: 0.2283\n",
      "Epoch 446/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0070Epoch 00445: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0069 - val_loss: 0.2283\n",
      "Epoch 447/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0069Epoch 00446: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0069 - val_loss: 0.2283\n",
      "Epoch 448/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00447: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2283\n",
      "Epoch 449/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0074Epoch 00448: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0073 - val_loss: 0.2283\n",
      "Epoch 450/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0080Epoch 00449: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0079 - val_loss: 0.2283\n",
      "Epoch 451/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0079Epoch 00450: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2282\n",
      "Epoch 452/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0081Epoch 00451: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0081 - val_loss: 0.2282\n",
      "Epoch 453/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0076\n",
      "Epoch 00452: reducing learning rate to 2.155053116439376e-05.\n",
      "Epoch 00452: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0076 - val_loss: 0.2283\n",
      "Epoch 454/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0066Epoch 00453: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0067 - val_loss: 0.2283\n",
      "Epoch 455/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0068Epoch 00454: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0068 - val_loss: 0.2285\n",
      "Epoch 456/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0072Epoch 00455: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0073 - val_loss: 0.2285\n",
      "Epoch 457/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0073Epoch 00456: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0074 - val_loss: 0.2284\n",
      "Epoch 458/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00457: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0074 - val_loss: 0.2284\n",
      "Epoch 459/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0086Epoch 00458: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0086 - val_loss: 0.2284\n",
      "Epoch 460/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0074Epoch 00459: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0074 - val_loss: 0.2284\n",
      "Epoch 461/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0058Epoch 00460: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0057 - val_loss: 0.2285\n",
      "Epoch 462/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0066Epoch 00461: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0067 - val_loss: 0.2285\n",
      "Epoch 463/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 00462: reducing learning rate to 1.9395478375372477e-05.\n",
      "Epoch 00462: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0079 - val_loss: 0.2286\n",
      "Epoch 464/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0076Epoch 00463: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2287\n",
      "Epoch 465/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00464: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2288\n",
      "Epoch 466/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0060Epoch 00465: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0060 - val_loss: 0.2288\n",
      "Epoch 467/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0062Epoch 00466: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0061 - val_loss: 0.2289\n",
      "Epoch 468/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0065Epoch 00467: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0065 - val_loss: 0.2290\n",
      "Epoch 469/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0074Epoch 00468: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0074 - val_loss: 0.2291\n",
      "Epoch 470/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0062Epoch 00469: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0062 - val_loss: 0.2291\n",
      "Epoch 471/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0078Epoch 00470: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0078 - val_loss: 0.2291\n",
      "Epoch 472/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0072Epoch 00471: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0072 - val_loss: 0.2292\n",
      "Epoch 473/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075\n",
      "Epoch 00472: reducing learning rate to 1.745593053783523e-05.\n",
      "Epoch 00472: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0074 - val_loss: 0.2293\n",
      "Epoch 474/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0076Epoch 00473: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0076 - val_loss: 0.2293\n",
      "Epoch 475/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0068Epoch 00474: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0068 - val_loss: 0.2293\n",
      "Epoch 476/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0067Epoch 00475: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0067 - val_loss: 0.2294\n",
      "Epoch 477/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00476: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2294\n",
      "Epoch 478/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0070Epoch 00477: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0070 - val_loss: 0.2295\n",
      "Epoch 479/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0070Epoch 00478: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259/259 [==============================] - 0s - loss: 0.0070 - val_loss: 0.2295\n",
      "Epoch 480/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0065Epoch 00479: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0065 - val_loss: 0.2295\n",
      "Epoch 481/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0063Epoch 00480: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0063 - val_loss: 0.2296\n",
      "Epoch 482/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0076Epoch 00481: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2296\n",
      "Epoch 483/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0065\n",
      "Epoch 00482: reducing learning rate to 1.5710336992924567e-05.\n",
      "Epoch 00482: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0065 - val_loss: 0.2297\n",
      "Epoch 484/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0068Epoch 00483: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0070 - val_loss: 0.2297\n",
      "Epoch 485/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0074Epoch 00484: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0074 - val_loss: 0.2297\n",
      "Epoch 486/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00485: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2297\n",
      "Epoch 487/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0068Epoch 00486: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0068 - val_loss: 0.2297\n",
      "Epoch 488/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0066Epoch 00487: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0066 - val_loss: 0.2296\n",
      "Epoch 489/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0059Epoch 00488: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0059 - val_loss: 0.2296\n",
      "Epoch 490/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0069Epoch 00489: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0068 - val_loss: 0.2296\n",
      "Epoch 491/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0064Epoch 00490: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0066 - val_loss: 0.2298\n",
      "Epoch 492/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0068Epoch 00491: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0068 - val_loss: 0.2300\n",
      "Epoch 493/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0085\n",
      "Epoch 00492: reducing learning rate to 1.4139303129923065e-05.\n",
      "Epoch 00492: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0087 - val_loss: 0.2304\n",
      "Epoch 494/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0082Epoch 00493: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0081 - val_loss: 0.2307\n",
      "Epoch 495/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00494: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2310\n",
      "Epoch 496/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0070Epoch 00495: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0070 - val_loss: 0.2312\n",
      "Epoch 497/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00496: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2314\n",
      "Epoch 498/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0066Epoch 00497: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0066 - val_loss: 0.2315\n",
      "Epoch 499/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0065Epoch 00498: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0066 - val_loss: 0.2316\n",
      "Epoch 500/500\n",
      "256/259 [============================>.] - ETA: 0s - loss: 0.0072Epoch 00499: val_loss did not improve\n",
      "259/259 [==============================] - 0s - loss: 0.0072 - val_loss: 0.2318\n"
     ]
    }
   ],
   "source": [
    "for layer in final_model.layers:\n",
    "    print (layer, layer.output_shape)\n",
    "\n",
    "try:\n",
    "    history = final_model.fit(X_train, Y_train, \n",
    "              epochs = 500, \n",
    "              batch_size = 256, \n",
    "              verbose=1, \n",
    "              validation_data=(X_test, Y_test),\n",
    "              callbacks=[reduce_lr, checkpointer],\n",
    "              shuffle=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print (e)\n",
    "finally:\n",
    "    final_model.load_weights(\"xxx.hdf5\")\n",
    "    pred = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = pred\n",
    "original = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_to_show = np.column_stack((closep, ma30, ma60))[WINDOW:]\n",
    "original_to_show = original_to_show[int(len(X) * TEST_SET):] - 80\n",
    "intersections = [0.]\n",
    "for i, (a, b) in enumerate(zip(original_to_show[:, 1], original_to_show[:, 2])):\n",
    "    if abs(a - b) < .5 and abs(i - intersections[-1]) > 5:\n",
    "        intersections.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff = abs(len(original_to_show) - len(predicted))\n",
    "zero_first = np.zeros((diff - FORECAST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original = remap(np.array(original), np.array(original).min(), np.array(original).max(), 0, 10)\n",
    "predicted = remap(np.array(predicted), np.array(predicted).min(), np.array(predicted).max(), 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original = np.append(zero_first, original)\n",
    "predicted = np.append(zero_first, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hussainmohammadyousuf/anaconda/lib/python3.6/site-packages/matplotlib/axes/_axes.py:531: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFeCAYAAABKNlxZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1263e77f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Actual and predicted')\n",
    "plt.legend(loc='best')\n",
    "plt.plot(original, color='black', label = 'Original data')\n",
    "plt.plot(original_to_show)\n",
    "\n",
    "indicator = original\n",
    "\n",
    "colors = ['green', 'orange', 'red']\n",
    "styles = ['-', '--']\n",
    "for i in intersections:\n",
    "    try:\n",
    "        if indicator[i] < 4:\n",
    "            color = colors[0]\n",
    "            style = styles[1]\n",
    "        elif indicator[i] >= 4 and indicator[i] < 7:\n",
    "            color = colors[1]\n",
    "            style = styles[0]\n",
    "        else:\n",
    "            color = colors[2]\n",
    "            style = styles[0]\n",
    "        plt.axvline(i, color = color, linestyle = style)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "plt.legend()\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.154183201967\n",
      "0.303965285687\n",
      "1.25863740066\n"
     ]
    }
   ],
   "source": [
    "print (np.mean(np.square(pred - Y_test)))\n",
    "print (np.mean(np.abs(pred - Y_test)))\n",
    "print (np.mean(np.abs((Y_test - pred) / Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
